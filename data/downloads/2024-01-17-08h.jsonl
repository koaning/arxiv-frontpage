{"created":"2024-01-16 14:45:41","title":"Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining","abstract":"Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting novel categories from a distinct domain using only limited exemplars. In this paper, we undertake a comprehensive study of CD-FSS and uncover two crucial insights: (i) the necessity of a fine-tuning stage to effectively transfer the learned meta-knowledge across domains, and (ii) the overfitting risk during the na\\\"ive fine-tuning due to the scarcity of novel category examples. With these insights, we propose a novel cross-domain fine-tuning strategy that addresses the challenging CD-FSS tasks. We first design Bi-directional Few-shot Prediction (BFP), which establishes support-query correspondence in a bi-directional manner, crafting augmented supervision to reduce the overfitting risk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which is a recursive framework to capture the support-query correspondence iteratively, targeting maximal exploitation of supervisory signals from the sparse novel category samples. Extensive empirical evaluations show that our method significantly outperforms the state-of-the-arts (+7.8\\%), which verifies that IFA tackles the cross-domain challenges and mitigates the overfitting simultaneously. Code will be made available.","sentences":["Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting novel categories from a distinct domain using only limited exemplars.","In this paper, we undertake a comprehensive study of CD-FSS and uncover two crucial insights: (i) the necessity of a fine-tuning stage to effectively transfer the learned meta-knowledge across domains, and (ii) the overfitting risk during the na\\\"ive fine-tuning due to the scarcity of novel category examples.","With these insights, we propose a novel cross-domain fine-tuning strategy that addresses the challenging CD-FSS tasks.","We first design Bi-directional Few-shot Prediction (BFP), which establishes support-query correspondence in a bi-directional manner, crafting augmented supervision to reduce the overfitting risk.","Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which is a recursive framework to capture the support-query correspondence iteratively, targeting maximal exploitation of supervisory signals from the sparse novel category samples.","Extensive empirical evaluations show that our method significantly outperforms the state-of-the-arts (+7.8\\%), which verifies that IFA tackles the cross-domain challenges and mitigates the overfitting simultaneously.","Code will be made available."],"url":"http://arxiv.org/abs/2401.08407v1"}
{"created":"2024-01-16 14:44:47","title":"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture","abstract":"There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.","sentences":["There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning.","RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself.","However, the pros and cons of both approaches are not well understood.","In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4.","Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results.","We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline.","We conduct an in-depth study on an agricultural dataset.","Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer?","Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning.","We see an accuracy increase of over 6 p.p.","when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further.","In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%.","Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains."],"url":"http://arxiv.org/abs/2401.08406v1"}
{"created":"2024-01-16 14:44:13","title":"Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT","abstract":"In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI as mere tools. Playful interactions with AI systems naturally emerged as a way for users to make sense of the ever-changing technology. However, these emergent and playful interactions are underexamined. We target this gap by investigating playful interactions exhibited by users of a recently trending powerful AI technology, ChatGPT. Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that a substantial portion of user discourse revolves around playful interactions. The analysis further allowed us to construct a preliminary taxonomy to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories. Overall, this study contributes to the field of HCI and CSCW by illuminating the multifaceted nature of playful interactions with AI, underlining their significance in shaping the human-AI relationship.","sentences":["In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI as mere tools.","Playful interactions with AI systems naturally emerged as a way for users to make sense of the ever-changing technology.","However, these emergent and playful interactions are underexamined.","We target this gap by investigating playful interactions exhibited by users of a recently trending powerful AI technology, ChatGPT.","Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that a substantial portion of user discourse revolves around playful interactions.","The analysis further allowed us to construct a preliminary taxonomy to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories.","Overall, this study contributes to the field of HCI and CSCW by illuminating the multifaceted nature of playful interactions with AI, underlining their significance in shaping the human-AI relationship."],"url":"http://arxiv.org/abs/2401.08405v1"}
{"created":"2024-01-16 14:43:26","title":"Uniform Recovery Guarantees for Quantized Corrupted Sensing Using Structured or Generative Priors","abstract":"This paper studies quantized corrupted sensing where the measurements are contaminated by unknown corruption and then quantized by a dithered uniform quantizer. We establish uniform guarantees for Lasso that ensure the accurate recovery of all signals and corruptions using a single draw of the sub-Gaussian sensing matrix and uniform dither. For signal and corruption with structured priors (e.g., sparsity, low-rankness), our uniform error rate for constrained Lasso typically coincides with the non-uniform one [Sun, Cui and Liu, 2022] up to logarithmic factors. By contrast, our uniform error rate for unconstrained Lasso exhibits worse dependence on the structured parameters due to regularization parameters larger than the ones for non-uniform recovery. For signal and corruption living in the ranges of some Lipschitz continuous generative models (referred to as generative priors), we achieve uniform recovery via constrained Lasso with a measurement number proportional to the latent dimensions of the generative models. Our treatments to the two kinds of priors are (nearly) unified and share the common key ingredients of (global) quantized product embedding (QPE) property, which states that the dithered uniform quantization (universally) preserves inner product. As a by-product, our QPE result refines the one in [Xu and Jacques, 2020] under sub-Gaussian random matrix, and in this specific instance we are able to sharpen the uniform error decaying rate (for the projected-back projection estimator with signals in some convex symmetric set) presented therein from $O(m^{-1/16})$ to $O(m^{-1/8})$.","sentences":["This paper studies quantized corrupted sensing where the measurements are contaminated by unknown corruption and then quantized by a dithered uniform quantizer.","We establish uniform guarantees for Lasso that ensure the accurate recovery of all signals and corruptions using a single draw of the sub-Gaussian sensing matrix and uniform dither.","For signal and corruption with structured priors (e.g., sparsity, low-rankness), our uniform error rate for constrained Lasso typically coincides with the non-uniform one [Sun, Cui and Liu, 2022] up to logarithmic factors.","By contrast, our uniform error rate for unconstrained Lasso exhibits worse dependence on the structured parameters due to regularization parameters larger than the ones for non-uniform recovery.","For signal and corruption living in the ranges of some Lipschitz continuous generative models (referred to as generative priors), we achieve uniform recovery via constrained Lasso with a measurement number proportional to the latent dimensions of the generative models.","Our treatments to the two kinds of priors are (nearly) unified and share the common key ingredients of (global) quantized product embedding (QPE) property, which states that the dithered uniform quantization (universally) preserves inner product.","As a by-product, our QPE result refines the one in [Xu and Jacques, 2020] under sub-Gaussian random matrix, and in this specific instance we are able to sharpen the uniform error decaying rate (for the projected-back projection estimator with signals in some convex symmetric set) presented therein from $O(m^{-1/16})$ to $O(m^{-1/8})$."],"url":"http://arxiv.org/abs/2401.08402v1"}
{"created":"2024-01-16 14:41:42","title":"TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding","abstract":"Humans commonly work with multiple objects in daily life and can intuitively transfer manipulation skills to novel objects by understanding object functional regularities. However, existing technical approaches for analyzing and synthesizing hand-object manipulation are mostly limited to handling a single hand and object due to the lack of data support. To address this, we construct TACO, an extensive bimanual hand-object-interaction dataset spanning a large variety of tool-action-object compositions for daily human activities. TACO contains 2.5K motion sequences paired with third-person and egocentric views, precise hand-object 3D meshes, and action labels. To rapidly expand the data scale, we present a fully-automatic data acquisition pipeline combining multi-view sensing with an optical motion capture system. With the vast research fields provided by TACO, we benchmark three generalizable hand-object-interaction tasks: compositional action recognition, generalizable hand-object motion forecasting, and cooperative grasp synthesis. Extensive experiments reveal new insights, challenges, and opportunities for advancing the studies of generalizable hand-object motion analysis and synthesis. Our data and code are available at https://taco2024.github.io.","sentences":["Humans commonly work with multiple objects in daily life and can intuitively transfer manipulation skills to novel objects by understanding object functional regularities.","However, existing technical approaches for analyzing and synthesizing hand-object manipulation are mostly limited to handling a single hand and object due to the lack of data support.","To address this, we construct TACO, an extensive bimanual hand-object-interaction dataset spanning a large variety of tool-action-object compositions for daily human activities.","TACO contains 2.5K motion sequences paired with third-person and egocentric views, precise hand-object 3D meshes, and action labels.","To rapidly expand the data scale, we present a fully-automatic data acquisition pipeline combining multi-view sensing with an optical motion capture system.","With the vast research fields provided by TACO, we benchmark three generalizable hand-object-interaction tasks: compositional action recognition, generalizable hand-object motion forecasting, and cooperative grasp synthesis.","Extensive experiments reveal new insights, challenges, and opportunities for advancing the studies of generalizable hand-object motion analysis and synthesis.","Our data and code are available at https://taco2024.github.io."],"url":"http://arxiv.org/abs/2401.08399v1"}
{"created":"2024-01-16 14:41:31","title":"High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering","abstract":"Readily editable mesh blendshapes have been widely used in animation pipelines, while recent advancements in neural geometry and appearance representations have enabled high-quality inverse rendering. Building upon these observations, we introduce a novel technique that reconstructs mesh-based blendshape rigs from single or sparse multi-view videos, leveraging state-of-the-art neural inverse rendering. We begin by constructing a deformation representation that parameterizes vertex displacements into differential coordinates with tetrahedral connections, allowing for high-quality vertex deformation on high-resolution meshes. By constructing a set of semantic regulations in this representation, we achieve joint optimization of blendshapes and expression coefficients. Furthermore, to enable a user-friendly multi-view setup with unsynchronized cameras, we propose a neural regressor to model time-varying motion parameters. This approach implicitly considers the time difference across multiple cameras, enhancing the accuracy of motion modeling. Experiments demonstrate that, with the flexible input of single or sparse multi-view videos, we reconstruct personalized high-fidelity blendshapes. These blendshapes are both geometrically and semantically accurate, and they are compatible with industrial animation pipelines. Code and data will be released.","sentences":["Readily editable mesh blendshapes have been widely used in animation pipelines, while recent advancements in neural geometry and appearance representations have enabled high-quality inverse rendering.","Building upon these observations, we introduce a novel technique that reconstructs mesh-based blendshape rigs from single or sparse multi-view videos, leveraging state-of-the-art neural inverse rendering.","We begin by constructing a deformation representation that parameterizes vertex displacements into differential coordinates with tetrahedral connections, allowing for high-quality vertex deformation on high-resolution meshes.","By constructing a set of semantic regulations in this representation, we achieve joint optimization of blendshapes and expression coefficients.","Furthermore, to enable a user-friendly multi-view setup with unsynchronized cameras, we propose a neural regressor to model time-varying motion parameters.","This approach implicitly considers the time difference across multiple cameras, enhancing the accuracy of motion modeling.","Experiments demonstrate that, with the flexible input of single or sparse multi-view videos, we reconstruct personalized high-fidelity blendshapes.","These blendshapes are both geometrically and semantically accurate, and they are compatible with industrial animation pipelines.","Code and data will be released."],"url":"http://arxiv.org/abs/2401.08398v1"}
{"created":"2024-01-16 14:41:20","title":"Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine","abstract":"Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows.","sentences":["Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks.","However, these evaluations primarily focused on the accuracy of multi-choice questions alone.","Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals.","Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034).","GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy.","However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%).","Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows."],"url":"http://arxiv.org/abs/2401.08396v1"}
{"created":"2024-01-16 14:41:20","title":"A Micro Architectural Events Aware Real-Time Embedded System Fault Injector","abstract":"In contemporary times, the increasing complexity of the system poses significant challenges to the reliability, trustworthiness, and security of the SACRES. Key issues include the susceptibility to phenomena such as instantaneous voltage spikes, electromagnetic interference, neutron strikes, and out-of-range temperatures. These factors can induce switch state changes in transistors, resulting in bit-flipping, soft errors, and transient corruption of stored data in memory. The occurrence of soft errors, in turn, may lead to system faults that can propel the system into a hazardous state. Particularly in critical sectors like automotive, avionics, or aerospace, such malfunctions can have real-world implications, potentially causing harm to individuals.   This paper introduces a novel fault injector designed to facilitate the monitoring, aggregation, and examination of micro-architectural events. This is achieved by harnessing the microprocessor's PMU and the debugging interface, specifically focusing on ensuring the repeatability of fault injections. The fault injection methodology targets bit-flipping within the memory system, affecting CPU registers and RAM. The outcomes of these fault injections enable a thorough analysis of the impact of soft errors and establish a robust correlation between the identified faults and the essential timing predictability demanded by SACRES.","sentences":["In contemporary times, the increasing complexity of the system poses significant challenges to the reliability, trustworthiness, and security of the SACRES.","Key issues include the susceptibility to phenomena such as instantaneous voltage spikes, electromagnetic interference, neutron strikes, and out-of-range temperatures.","These factors can induce switch state changes in transistors, resulting in bit-flipping, soft errors, and transient corruption of stored data in memory.","The occurrence of soft errors, in turn, may lead to system faults that can propel the system into a hazardous state.","Particularly in critical sectors like automotive, avionics, or aerospace, such malfunctions can have real-world implications, potentially causing harm to individuals.   ","This paper introduces a novel fault injector designed to facilitate the monitoring, aggregation, and examination of micro-architectural events.","This is achieved by harnessing the microprocessor's PMU and the debugging interface, specifically focusing on ensuring the repeatability of fault injections.","The fault injection methodology targets bit-flipping within the memory system, affecting CPU registers and RAM.","The outcomes of these fault injections enable a thorough analysis of the impact of soft errors and establish a robust correlation between the identified faults and the essential timing predictability demanded by SACRES."],"url":"http://arxiv.org/abs/2401.08397v1"}
{"created":"2024-01-16 14:33:09","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models","abstract":"The field of AI agents is advancing at an unprecedented rate due to the capabilities of large language models (LLMs). However, LLM-driven visual agents mainly focus on solving tasks for the image modality, which limits their ability to understand the dynamic nature of the real world, making it still far from real-life applications, e.g., guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing and perceptually intensive nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video with massive content into a symbolic memory that stores \\textit{task-related} attributes. This structured representation allows for spatial-temporal querying and reasoning by sub-task tools, resulting in concise and relevant intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorporate plug-and-play tools to assess external knowledge and address tasks across different domains. Moreover, we introduce a novel LLM-driven planner based on Monte Carlo Tree Search to efficiently explore the large planning space for scheduling various tools. The planner iteratively finds feasible solutions by backpropagating the result's reward, and multiple solutions can be summarized into an improved final answer. We extensively evaluate DoraemonGPT in dynamic scenes and provide in-the-wild showcases demonstrating its ability to handle more complex questions than previous studies.","sentences":["The field of AI agents is advancing at an unprecedented rate due to the capabilities of large language models (LLMs).","However, LLM-driven visual agents mainly focus on solving tasks for the image modality, which limits their ability to understand the dynamic nature of the real world, making it still far from real-life applications, e.g., guiding students in laboratory experiments and identifying their mistakes.","Considering the video modality better reflects the ever-changing and perceptually intensive nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks.","Given a video with a question/task, DoraemonGPT begins by converting the input video with massive content into a symbolic memory that stores \\textit{task-related} attributes.","This structured representation allows for spatial-temporal querying and reasoning by sub-task tools, resulting in concise and relevant intermediate results.","Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorporate plug-and-play tools to assess external knowledge and address tasks across different domains.","Moreover, we introduce a novel LLM-driven planner based on Monte Carlo Tree Search to efficiently explore the large planning space for scheduling various tools.","The planner iteratively finds feasible solutions by backpropagating the result's reward, and multiple solutions can be summarized into an improved final answer.","We extensively evaluate DoraemonGPT in dynamic scenes and provide in-the-wild showcases demonstrating its ability to handle more complex questions than previous studies."],"url":"http://arxiv.org/abs/2401.08392v1"}
{"created":"2024-01-16 14:19:28","title":"Deep Learning-based Group Causal Inference in Multivariate Time-series","abstract":"Causal inference in a nonlinear system of multivariate timeseries is instrumental in disentangling the intricate web of relationships among variables, enabling us to make more accurate predictions and gain deeper insights into real-world complex systems. Causality methods typically identify the causal structure of a multivariate system by considering the cause-effect relationship of each pair of variables while ignoring the collective effect of a group of variables or interactions involving more than two-time series variables. In this work, we test model invariance by group-level interventions on the trained deep networks to infer causal direction in groups of variables, such as climate and ecosystem, brain networks, etc. Extensive testing with synthetic and real-world time series data shows a significant improvement of our method over other applied group causality methods and provides us insights into real-world time series. The code for our method can be found at:https://github.com/wasimahmadpk/gCause.","sentences":["Causal inference in a nonlinear system of multivariate timeseries is instrumental in disentangling the intricate web of relationships among variables, enabling us to make more accurate predictions and gain deeper insights into real-world complex systems.","Causality methods typically identify the causal structure of a multivariate system by considering the cause-effect relationship of each pair of variables while ignoring the collective effect of a group of variables or interactions involving more than two-time series variables.","In this work, we test model invariance by group-level interventions on the trained deep networks to infer causal direction in groups of variables, such as climate and ecosystem, brain networks, etc.","Extensive testing with synthetic and real-world time series data shows a significant improvement of our method over other applied group causality methods and provides us insights into real-world time series.","The code for our method can be found at:https://github.com/wasimahmadpk/gCause."],"url":"http://arxiv.org/abs/2401.08386v1"}
{"created":"2024-01-16 14:19:04","title":"An Efficient VCGen-based Modular Verification of Relational Properties","abstract":"Deductive verification typically relies on function contracts that specify the behavior of each function for a single function call. Relational properties link several function calls together within a single specification. They can express more advanced properties of a given function, such as non-interference, continuity, or monotonicity, or relate calls to different functions, possibly run in parallel, for instance, to show the equivalence of two implementations. However, relational properties cannot be expressed and verified directly in the traditional setting of modular deductive verification. Recent work proposed a new technique for relational property verification that relies on a verification condition generator to produce logical formulas that must be verified to ensure a given relational property. This paper presents an overview of this approach and proposes important enhancements. We integrate an optimized verification condition generator and extend the underlying theory to show how relational properties can be proved in a modular way, where one relational property can be used to prove another one, like in modular verification of function contracts. Our results have been fully formalized and proved sound in the Coq proof assistant.","sentences":["Deductive verification typically relies on function contracts that specify the behavior of each function for a single function call.","Relational properties link several function calls together within a single specification.","They can express more advanced properties of a given function, such as non-interference, continuity, or monotonicity, or relate calls to different functions, possibly run in parallel, for instance, to show the equivalence of two implementations.","However, relational properties cannot be expressed and verified directly in the traditional setting of modular deductive verification.","Recent work proposed a new technique for relational property verification that relies on a verification condition generator to produce logical formulas that must be verified to ensure a given relational property.","This paper presents an overview of this approach and proposes important enhancements.","We integrate an optimized verification condition generator and extend the underlying theory to show how relational properties can be proved in a modular way, where one relational property can be used to prove another one, like in modular verification of function contracts.","Our results have been fully formalized and proved sound in the Coq proof assistant."],"url":"http://arxiv.org/abs/2401.08385v1"}
{"created":"2024-01-16 14:16:47","title":"Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference","abstract":"In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism on distributed systems, our design only uses one Alltoall communication to deliver the same functionality while previous methods all require two Alltoalls. By carefully examining the conditional probability in tokens' routing across multiple layers, we proved that pre-trained GPT MoE models implicitly exhibit a strong inter-layer expert affinity. We then design an efficient integer programming model to capture such features and show that by properly placing the experts on corresponding GPUs, we can reduce up to 67% cross-GPU routing latency. Our solution beats the cutting-edge MoE implementations with experts from 8 to 64, with up to 2.2x improvement in inference throughput. We further provide a detailed study of how the model implicitly acquires this expert affinity at the very early training stage and how this affinity evolves and stabilizes during training.","sentences":["In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy.","However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation.","This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources.","In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models.","We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity.","Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation.","By proposing a context-coherent expert parallelism on distributed systems, our design only uses one Alltoall communication to deliver the same functionality while previous methods all require two Alltoalls.","By carefully examining the conditional probability in tokens' routing across multiple layers, we proved that pre-trained GPT MoE models implicitly exhibit a strong inter-layer expert affinity.","We then design an efficient integer programming model to capture such features and show that by properly placing the experts on corresponding GPUs, we can reduce up to 67% cross-GPU routing latency.","Our solution beats the cutting-edge MoE implementations with experts from 8 to 64, with up to 2.2x improvement in inference throughput.","We further provide a detailed study of how the model implicitly acquires this expert affinity at the very early training stage and how this affinity evolves and stabilizes during training."],"url":"http://arxiv.org/abs/2401.08383v1"}
{"created":"2024-01-16 14:11:54","title":"Robotic Imitation of Human Actions","abstract":"Imitation can allow us to quickly gain an understanding of a new task. Through a demonstration, we can gain direct knowledge about which actions need to be performed and which goals they have. In this paper, we introduce a new approach to imitation learning that tackles the challenges of a robot imitating a human, such as the change in perspective and body schema. Our approach can use a single human demonstration to abstract information about the demonstrated task, and use that information to generalise and replicate it. We facilitate this ability by a new integration of two state-of-the-art methods: a diffusion action segmentation model to abstract temporal information from the demonstration and an open vocabulary object detector for spatial information. Furthermore, we refine the abstracted information and use symbolic reasoning to create an action plan utilising inverse kinematics, to allow the robot to imitate the demonstrated action.","sentences":["Imitation can allow us to quickly gain an understanding of a new task.","Through a demonstration, we can gain direct knowledge about which actions need to be performed and which goals they have.","In this paper, we introduce a new approach to imitation learning that tackles the challenges of a robot imitating a human, such as the change in perspective and body schema.","Our approach can use a single human demonstration to abstract information about the demonstrated task, and use that information to generalise and replicate it.","We facilitate this ability by a new integration of two state-of-the-art methods: a diffusion action segmentation model to abstract temporal information from the demonstration and an open vocabulary object detector for spatial information.","Furthermore, we refine the abstracted information and use symbolic reasoning to create an action plan utilising inverse kinematics, to allow the robot to imitate the demonstrated action."],"url":"http://arxiv.org/abs/2401.08381v1"}
{"created":"2024-01-16 14:07:48","title":"KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation","abstract":"Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods. Our source code and data are available at https://github.com/DeepSoftwareAnalytics/KADEL","sentences":["Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance.","However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not.","On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation.","Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL.","Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits.","To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits.","This knowledge model enables supplementing more information for training samples that do not conform to good practice.","However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method.","This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process.","Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods.","Our source code and data are available at https://github.com/DeepSoftwareAnalytics/KADEL"],"url":"http://arxiv.org/abs/2401.08376v1"}
{"created":"2024-01-16 14:00:28","title":"Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation","abstract":"Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators. However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation tasks. In this paper, we introduce a novel neural approach aimed at overcoming this limitation by exploiting not only TMs, but also in-domain target-language (TL) monolingual corpora, and still enabling a similar functionality to that offered by conventional TM-based CAT tools. Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora, and on a neural model to estimate their post-editing effort. The paper presents an automatic evaluation of these techniques on four language pairs that shows that our approach can successfully exploit monolingual texts in a TM-based CAT environment, increasing the amount of useful translation proposals, and that our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way. A human evaluation performed on a single language pair confirms the results of the automatic evaluation and seems to indicate that the translation proposals retrieved with our approach are more useful than what the automatic evaluation shows.","sentences":["Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators.","However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation tasks.","In this paper, we introduce a novel neural approach aimed at overcoming this limitation by exploiting not only TMs, but also in-domain target-language (TL) monolingual corpora, and still enabling a similar functionality to that offered by conventional TM-based CAT tools.","Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora, and on a neural model to estimate their post-editing effort.","The paper presents an automatic evaluation of these techniques on four language pairs that shows that our approach can successfully exploit monolingual texts in a TM-based CAT environment, increasing the amount of useful translation proposals, and that our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way.","A human evaluation performed on a single language pair confirms the results of the automatic evaluation and seems to indicate that the translation proposals retrieved with our approach are more useful than what the automatic evaluation shows."],"url":"http://arxiv.org/abs/2401.08374v1"}
{"created":"2024-01-16 13:52:25","title":"Morphology and Syntax of the Tamil Language","abstract":"This paper provides an overview of the morphology and syntax of the Tamil language, focusing on its contemporary usage. The paper also highlights the complexity and richness of Tamil in terms of its morphological and syntactic features, which will be useful for linguists analysing the language and conducting comparative studies. In addition, the paper will be useful for those developing computational resources for the Tamil language. It is proven as a rule-based morphological analyser cum generator and a computational grammar for Tamil have already been developed based on this paper. To enhance accessibility for a broader audience, the analysis is conducted without relying on any specific grammatical formalism.","sentences":["This paper provides an overview of the morphology and syntax of the Tamil language, focusing on its contemporary usage.","The paper also highlights the complexity and richness of Tamil in terms of its morphological and syntactic features, which will be useful for linguists analysing the language and conducting comparative studies.","In addition, the paper will be useful for those developing computational resources for the Tamil language.","It is proven as a rule-based morphological analyser cum generator and a computational grammar for Tamil have already been developed based on this paper.","To enhance accessibility for a broader audience, the analysis is conducted without relying on any specific grammatical formalism."],"url":"http://arxiv.org/abs/2401.08367v1"}
{"created":"2024-01-16 13:48:35","title":"On the formalization of the notion of an algorithm","abstract":"The starting point of this paper is a collection of properties of an algorithm that have been distilled from the informal descriptions of what an algorithm is that are given in standard works from the mathematical and computer science literature. Based on that, the notion of a proto-algorithm is introduced. The thought is that algorithms are equivalence classes of proto-algorithms under some equivalence relation. Three equivalence relations are defined. Two of them give bounds between which an appropriate equivalence relation must lie. The third lies in between these two and is likely an appropriate equivalence relation. A sound method is presented to prove, using an imperative process algebra based on ACP, that this equivalence relation holds between two proto-algorithms.","sentences":["The starting point of this paper is a collection of properties of an algorithm that have been distilled from the informal descriptions of what an algorithm is that are given in standard works from the mathematical and computer science literature.","Based on that, the notion of a proto-algorithm is introduced.","The thought is that algorithms are equivalence classes of proto-algorithms under some equivalence relation.","Three equivalence relations are defined.","Two of them give bounds between which an appropriate equivalence relation must lie.","The third lies in between these two and is likely an appropriate equivalence relation.","A sound method is presented to prove, using an imperative process algebra based on ACP, that this equivalence relation holds between two proto-algorithms."],"url":"http://arxiv.org/abs/2401.08366v1"}
{"created":"2024-01-16 13:46:10","title":"Weighted Spectral Filters for Kernel Interpolation on Spheres: Estimates of Prediction Accuracy for Noisy Data","abstract":"Spherical radial-basis-based kernel interpolation abounds in image sciences including geophysical image reconstruction, climate trends description and image rendering due to its excellent spatial localization property and perfect approximation performance. However, in dealing with noisy data, kernel interpolation frequently behaves not so well due to the large condition number of the kernel matrix and instability of the interpolation process. In this paper, we introduce a weighted spectral filter approach to reduce the condition number of the kernel matrix and then stabilize kernel interpolation. The main building blocks of the proposed method are the well developed spherical positive quadrature rules and high-pass spectral filters. Using a recently developed integral operator approach for spherical data analysis, we theoretically demonstrate that the proposed weighted spectral filter approach succeeds in breaking through the bottleneck of kernel interpolation, especially in fitting noisy data. We provide optimal approximation rates of the new method to show that our approach does not compromise the predicting accuracy. Furthermore, we conduct both toy simulations and two real-world data experiments with synthetically added noise in geophysical image reconstruction and climate image processing to verify our theoretical assertions and show the feasibility of the weighted spectral filter approach.","sentences":["Spherical radial-basis-based kernel interpolation abounds in image sciences including geophysical image reconstruction, climate trends description and image rendering due to its excellent spatial localization property and perfect approximation performance.","However, in dealing with noisy data, kernel interpolation frequently behaves not so well due to the large condition number of the kernel matrix and instability of the interpolation process.","In this paper, we introduce a weighted spectral filter approach to reduce the condition number of the kernel matrix and then stabilize kernel interpolation.","The main building blocks of the proposed method are the well developed spherical positive quadrature rules and high-pass spectral filters.","Using a recently developed integral operator approach for spherical data analysis, we theoretically demonstrate that the proposed weighted spectral filter approach succeeds in breaking through the bottleneck of kernel interpolation, especially in fitting noisy data.","We provide optimal approximation rates of the new method to show that our approach does not compromise the predicting accuracy.","Furthermore, we conduct both toy simulations and two real-world data experiments with synthetically added noise in geophysical image reconstruction and climate image processing to verify our theoretical assertions and show the feasibility of the weighted spectral filter approach."],"url":"http://arxiv.org/abs/2401.08364v1"}
{"created":"2024-01-16 13:45:54","title":"Mitigating Bias in Machine Learning Models for Phishing Webpage Detection","abstract":"The widespread accessibility of the Internet has led to a surge in online fraudulent activities, underscoring the necessity of shielding users' sensitive information from cybercriminals. Phishing, a well-known cyberattack, revolves around the creation of phishing webpages and the dissemination of corresponding URLs, aiming to deceive users into sharing their sensitive information, often for identity theft or financial gain. Various techniques are available for preemptively categorizing zero-day phishing URLs by distilling unique attributes and constructing predictive models. However, these existing techniques encounter unresolved issues. This proposal delves into persistent challenges within phishing detection solutions, particularly concentrated on the preliminary phase of assembling comprehensive datasets, and proposes a potential solution in the form of a tool engineered to alleviate bias in ML models. Such a tool can generate phishing webpages for any given set of legitimate URLs, infusing randomly selected content and visual-based phishing features. Furthermore, we contend that the tool holds the potential to assess the efficacy of existing phishing detection solutions, especially those trained on confined datasets.","sentences":["The widespread accessibility of the Internet has led to a surge in online fraudulent activities, underscoring the necessity of shielding users' sensitive information from cybercriminals.","Phishing, a well-known cyberattack, revolves around the creation of phishing webpages and the dissemination of corresponding URLs, aiming to deceive users into sharing their sensitive information, often for identity theft or financial gain.","Various techniques are available for preemptively categorizing zero-day phishing URLs by distilling unique attributes and constructing predictive models.","However, these existing techniques encounter unresolved issues.","This proposal delves into persistent challenges within phishing detection solutions, particularly concentrated on the preliminary phase of assembling comprehensive datasets, and proposes a potential solution in the form of a tool engineered to alleviate bias in ML models.","Such a tool can generate phishing webpages for any given set of legitimate URLs, infusing randomly selected content and visual-based phishing features.","Furthermore, we contend that the tool holds the potential to assess the efficacy of existing phishing detection solutions, especially those trained on confined datasets."],"url":"http://arxiv.org/abs/2401.08363v1"}
{"created":"2024-01-16 13:37:45","title":"AdaSem: Adaptive Goal-Oriented Semantic Communications for End-to-End Camera Relocalization","abstract":"Recently, deep autoencoders have gained traction as a powerful method for implementing goal-oriented semantic communications systems. The idea is to train a mapping from the source domain directly to channel symbols, and vice versa. However, prior studies often focused on rate-distortion tradeoff and transmission delay, at the cost of increasing end-to-end complexity and thus latency. Moreover, the datasets used are often not reflective of real-world environments, and the results were not validated against real-world baseline systems, leading to an unfair comparison. In this paper, we study the problem of remote camera pose estimation and propose AdaSem, an adaptive semantic communications approach that optimizes the tradeoff between inference accuracy and end-to-end latency. We develop an adaptive semantic codec model, which encodes the source data into a dynamic number of symbols, based on the latent space distribution and the channel state feedback. We utilize a lightweight model for both transmitter and receiver to ensure comparable complexity to the baseline implemented in a real-world system. Extensive experiments on real-environment data show the effectiveness of our approach. When compared to a real implementation of a client-server camera relocalization service, AdaSem outperforms the baseline by reducing the end-to-end delay and estimation error by over 75% and 63%, respectively.","sentences":["Recently, deep autoencoders have gained traction as a powerful method for implementing goal-oriented semantic communications systems.","The idea is to train a mapping from the source domain directly to channel symbols, and vice versa.","However, prior studies often focused on rate-distortion tradeoff and transmission delay, at the cost of increasing end-to-end complexity and thus latency.","Moreover, the datasets used are often not reflective of real-world environments, and the results were not validated against real-world baseline systems, leading to an unfair comparison.","In this paper, we study the problem of remote camera pose estimation and propose AdaSem, an adaptive semantic communications approach that optimizes the tradeoff between inference accuracy and end-to-end latency.","We develop an adaptive semantic codec model, which encodes the source data into a dynamic number of symbols, based on the latent space distribution and the channel state feedback.","We utilize a lightweight model for both transmitter and receiver to ensure comparable complexity to the baseline implemented in a real-world system.","Extensive experiments on real-environment data show the effectiveness of our approach.","When compared to a real implementation of a client-server camera relocalization service, AdaSem outperforms the baseline by reducing the end-to-end delay and estimation error by over 75% and 63%, respectively."],"url":"http://arxiv.org/abs/2401.08360v1"}
{"created":"2024-01-16 13:36:07","title":"Hallucination Detection and Hallucination Mitigation: An Investigation","abstract":"Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.","sentences":["Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications.","In spite of these successes, there exist concerns that limit the wide application of LLMs.","A key problem is the problem of hallucination.","Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses.","This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation.","We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks."],"url":"http://arxiv.org/abs/2401.08358v1"}
{"created":"2024-01-16 13:35:28","title":"SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection","abstract":"Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately. To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability. First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels. To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy. Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results. Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations. The source code is available at https://github.com/ixilai/SAMF.","sentences":["Existing multi-focus image fusion (MFIF) methods often fail to preserve the uncertain transition region and detect small focus areas within large defocused regions accurately.","To address this issue, this study proposes a new small-area-aware MFIF algorithm for enhancing object detection capability.","First, we enhance the pixel attributes within the small focus and boundary regions, which are subsequently combined with visual saliency detection to obtain the pre-fusion results used to discriminate the distribution of focused pixels.","To accurately ensure pixel focus, we consider the source image as a combination of focused, defocused, and uncertain regions and propose a three-region segmentation strategy.","Finally, we design an effective pixel selection rule to generate segmentation decision maps and obtain the final fusion results.","Experiments demonstrated that the proposed method can accurately detect small and smooth focus areas while improving object detection performance, outperforming existing methods in both subjective and objective evaluations.","The source code is available at https://github.com/ixilai/SAMF."],"url":"http://arxiv.org/abs/2401.08357v1"}
{"created":"2024-01-16 13:30:37","title":"Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach","abstract":"Federated learning aims to infer a shared model from private and decentralized data stored locally by multiple clients. Personalized federated learning (PFL) goes one step further by adapting the global model to each client, enhancing the model's fit for different clients. A significant level of personalization is required for highly heterogeneous clients, but can be challenging to achieve especially when they have small datasets. To address this problem, we propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework that utilizes differential privacy to handle data-dependent priors. Our algorithm collaboratively learns a shared hyper-posterior and regards each client's posterior inference as the personalization step. By establishing and minimizing a generalization bound on the average true risk of clients, PAC-PFL effectively combats over-fitting. PACPFL achieves accurate and well-calibrated predictions, supported by experiments on a dataset of photovoltaic panel power generation, FEMNIST dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen et al., 2017).","sentences":["Federated learning aims to infer a shared model from private and decentralized data stored locally by multiple clients.","Personalized federated learning (PFL) goes one step further by adapting the global model to each client, enhancing the model's fit for different clients.","A significant level of personalization is required for highly heterogeneous clients, but can be challenging to achieve especially when they have small datasets.","To address this problem, we propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework that utilizes differential privacy to handle data-dependent priors.","Our algorithm collaboratively learns a shared hyper-posterior and regards each client's posterior inference as the personalization step.","By establishing and minimizing a generalization bound on the average true risk of clients, PAC-PFL effectively combats over-fitting.","PACPFL achieves accurate and well-calibrated predictions, supported by experiments on a dataset of photovoltaic panel power generation, FEMNIST dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen et al., 2017)."],"url":"http://arxiv.org/abs/2401.08351v1"}
{"created":"2024-01-16 13:30:09","title":"Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models","abstract":"The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase. Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words. However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist. While the challenges of word alignment and beam search, specifically associated with NMT, may not apply to LLMs, we identify three new challenges for LLMs in translation tasks: inference efficiency, translation of low-resource languages in the pretraining phase, and human-aligned evaluation. The datasets and models are released at https://github.com/pangjh3/LLM4MT.","sentences":["The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field.","This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search.","Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase.","Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words.","However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist.","While the challenges of word alignment and beam search, specifically associated with NMT, may not apply to LLMs, we identify three new challenges for LLMs in translation tasks: inference efficiency, translation of low-resource languages in the pretraining phase, and human-aligned evaluation.","The datasets and models are released at https://github.com/pangjh3/LLM4MT."],"url":"http://arxiv.org/abs/2401.08350v1"}
{"created":"2024-01-16 13:29:30","title":"We don't need no labels: Estimating post-deployment model performance under covariate shift without ground truth","abstract":"The performance of machine learning models often degrades after deployment due to data distribution shifts. In many use cases, it is impossible to calculate the post-deployment performance because labels are unavailable or significantly delayed. Proxy methods for evaluating model performance stability, like drift detection techniques, do not properly quantify data distribution shift impact. As a solution, we propose a robust and accurate performance estimation method for evaluating ML classification models on unlabeled data that accurately quantifies the impact of covariate shift on model performance. We call it multi-calibrated confidence-based performance estimation (M-CBPE). It is model and data-type agnostic and works for any performance metric. It does not require access to the monitored model - it uses the model predictions and probability estimates. M-CBPE does not need user input on the nature of the covariate shift as it fully learns from the data. We evaluate it with over 600 dataset-model pairs from US census data and compare it with multiple benchmarks using several evaluation metrics. Results show that M-CBPE is the best method to estimate the performance of classification models in any evaluation context.","sentences":["The performance of machine learning models often degrades after deployment due to data distribution shifts.","In many use cases, it is impossible to calculate the post-deployment performance because labels are unavailable or significantly delayed.","Proxy methods for evaluating model performance stability, like drift detection techniques, do not properly quantify data distribution shift impact.","As a solution, we propose a robust and accurate performance estimation method for evaluating ML classification models on unlabeled data that accurately quantifies the impact of covariate shift on model performance.","We call it multi-calibrated confidence-based performance estimation (M-CBPE).","It is model and data-type agnostic and works for any performance metric.","It does not require access to the monitored model - it uses the model predictions and probability estimates.","M-CBPE does not need user input on the nature of the covariate shift as it fully learns from the data.","We evaluate it with over 600 dataset-model pairs from US census data and compare it with multiple benchmarks using several evaluation metrics.","Results show that M-CBPE is the best method to estimate the performance of classification models in any evaluation context."],"url":"http://arxiv.org/abs/2401.08348v1"}
{"created":"2024-01-16 13:23:51","title":"Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition(CLIP-$\\mathrm{M^2}$DF)","abstract":"In recent years, few-shot action recognition has attracted increasing attention. It generally adopts the paradigm of meta-learning. In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples. We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity. Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion. Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query. Secondly, we establish a Multi-view. In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers. Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias. Our code is available at the URL: \\url{https://github.com/cofly2014/MDMF}.","sentences":["In recent years, few-shot action recognition has attracted increasing attention.","It generally adopts the paradigm of meta-learning.","In this field, overcoming the overlapping distribution of classes and outliers is still a challenging problem based on limited samples.","We believe the combination of Multi-modal and Multi-view can improve this issue depending on information complementarity.","Therefore, we propose a method of Multi-view Distillation based on Multi-modal Fusion.","Firstly, a Probability Prompt Selector for the query is constructed to generate probability prompt embedding based on the comparison score between the prompt embeddings of the support and the visual embedding of the query.","Secondly, we establish a Multi-view.","In each view, we fuse the prompt embedding as consistent information with visual and the global or local temporal context to overcome the overlapping distribution of classes and outliers.","Thirdly, we perform the distance fusion for the Multi-view and the mutual distillation of matching ability from one to another, enabling the model to be more robust to the distribution bias.","Our code is available at the URL: \\url{https://github.com/cofly2014/MDMF}."],"url":"http://arxiv.org/abs/2401.08345v1"}
{"created":"2024-01-16 13:17:33","title":"Direct-Conflict Resolution in Intent-Driven Autonomous Networks","abstract":"As network systems evolve, there is an escalating demand for automated tools to facilitate efficient management and configuration. This paper explores conflict resolution in Intent-Based Network (IBN) management, an innovative approach that holds promise for effective network administration, especially within radio access domain. Nevertheless, when multiple intents are in operation concurrently, conflicts may emerge, presenting a significant issue that remains under-addressed in the current literature. In response to this challenge, our research expands the range of conflict resolution strategies beyond the established Nash Bargaining Solution (NBS), to incorporate the Weighted Nash Bargaining Solution (WNBS), the Kalai-Smorodinsky Bargaining Solution (KSBS), and the Shannon Entropy Bargaining Solution (SEBS). These methods are employed with the objective to identify optimal parameter values, aiming to ensure fairness in conflict resolution. Through simulations, it is demonstrated that distinct antenna tilt values are yielded as the respective solutions for each method. Ultimately, based on Jain Fairness Index, the KSBS is identified as the most equitable method under the given conditions.","sentences":["As network systems evolve, there is an escalating demand for automated tools to facilitate efficient management and configuration.","This paper explores conflict resolution in Intent-Based Network (IBN) management, an innovative approach that holds promise for effective network administration, especially within radio access domain.","Nevertheless, when multiple intents are in operation concurrently, conflicts may emerge, presenting a significant issue that remains under-addressed in the current literature.","In response to this challenge, our research expands the range of conflict resolution strategies beyond the established Nash Bargaining Solution (NBS), to incorporate the Weighted Nash Bargaining Solution (WNBS), the Kalai-Smorodinsky Bargaining Solution (KSBS), and the Shannon Entropy Bargaining Solution (SEBS).","These methods are employed with the objective to identify optimal parameter values, aiming to ensure fairness in conflict resolution.","Through simulations, it is demonstrated that distinct antenna tilt values are yielded as the respective solutions for each method.","Ultimately, based on Jain Fairness Index, the KSBS is identified as the most equitable method under the given conditions."],"url":"http://arxiv.org/abs/2401.08341v1"}
{"created":"2024-01-16 12:57:35","title":"dabih -- encrypted data storage and sharing platform","abstract":"Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research. Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.   Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management. dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format. Its approach to data security involves a two-stage envelope encryption process. We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism. The private key necessary for decrypting the data remains exclusively on the owner's device. Thus, accessing data is impossible without explicit permission from the keyholder.   Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries. Documentation is available as part of the web application.   Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions. All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files.","sentences":["Background: The secure management of sensitive clinical data, particularly human genomics data, has become a critical requirement in modern biomedical research.","Although the necessary software and algorithms are readily available, their use by non-IT experts poses significant challenges.   ","Methods: We developed dabih, an open-source web application specifically designed to facilitate user-friendly encrypted data management.","dabih enables web-based uploading, storing, sharing, and downloading of sensitive data in any format.","Its approach to data security involves a two-stage envelope encryption process.","We combine symmetric-key encryption for data and public-key encryption as key encapsulation mechanism.","The private key necessary for decrypting the data remains exclusively on the owner's device.","Thus, accessing data is impossible without explicit permission from the keyholder.   ","Results: dabih is available open-source on GitHub https://github.com/spang-lab/dabih, as ready to use containers on docker hub and includes a command line interface and a graphical bulk upload tool as pre-built binaries.","Documentation is available as part of the web application.   ","Conclusions: dabih enables everyone to use strong cryptography for their data, while being just as simple to use as other, non-encrypted, data storage solutions.","All the cryptography occurs seamlessly in the background as users interact with a secure web portal, simply by dragging and dropping files."],"url":"http://arxiv.org/abs/2401.08333v1"}
{"created":"2024-01-16 12:53:42","title":"Generative Denoise Distillation: Simple Stochastic Noises Induce Efficient Knowledge Transfer for Dense Prediction","abstract":"Knowledge distillation is the process of transferring knowledge from a more powerful large model (teacher) to a simpler counterpart (student). Numerous current approaches involve the student imitating the knowledge of the teacher directly. However, redundancy still exists in the learned representations through these prevalent methods, which tend to learn each spatial location's features indiscriminately. To derive a more compact representation (concept feature) from the teacher, inspired by human cognition, we suggest an innovative method, termed Generative Denoise Distillation (GDD), where stochastic noises are added to the concept feature of the student to embed them into the generated instance feature from a shallow network. Then, the generated instance feature is aligned with the knowledge of the instance from the teacher. We extensively experiment with object detection, instance segmentation, and semantic segmentation to demonstrate the versatility and effectiveness of our method. Notably, GDD achieves new state-of-the-art performance in the tasks mentioned above. We have achieved substantial improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69, respectively, surpassing their previous scores of 69.85 and 73.20 on the Cityscapes dataset of 20 categories. The source code of GDD is available at https://github.com/ZhgLiu/GDD.","sentences":["Knowledge distillation is the process of transferring knowledge from a more powerful large model (teacher) to a simpler counterpart (student).","Numerous current approaches involve the student imitating the knowledge of the teacher directly.","However, redundancy still exists in the learned representations through these prevalent methods, which tend to learn each spatial location's features indiscriminately.","To derive a more compact representation (concept feature) from the teacher, inspired by human cognition, we suggest an innovative method, termed Generative Denoise Distillation (GDD), where stochastic noises are added to the concept feature of the student to embed them into the generated instance feature from a shallow network.","Then, the generated instance feature is aligned with the knowledge of the instance from the teacher.","We extensively experiment with object detection, instance segmentation, and semantic segmentation to demonstrate the versatility and effectiveness of our method.","Notably, GDD achieves new state-of-the-art performance in the tasks mentioned above.","We have achieved substantial improvements in semantic segmentation by enhancing PspNet and DeepLabV3, both of which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69, respectively, surpassing their previous scores of 69.85 and 73.20 on the Cityscapes dataset of 20 categories.","The source code of GDD is available at https://github.com/ZhgLiu/GDD."],"url":"http://arxiv.org/abs/2401.08332v1"}
{"created":"2024-01-16 12:49:10","title":"Boosting Gradient Ascent for Continuous DR-submodular Maximization","abstract":"Projected Gradient Ascent (PGA) is the most commonly used optimization scheme in machine learning and operations research areas. Nevertheless, numerous studies and examples have shown that the PGA methods may fail to achieve the tight approximation ratio for continuous DR-submodular maximization problems. To address this challenge, we present a boosting technique in this paper, which can efficiently improve the approximation guarantee of the standard PGA to \\emph{optimal} with only small modifications on the objective function. The fundamental idea of our boosting technique is to exploit non-oblivious search to derive a novel auxiliary function $F$, whose stationary points are excellent approximations to the global maximum of the original DR-submodular objective $f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we propose an auxiliary function $F$ whose stationary points can provide a better $(1-e^{-\\gamma})$-approximation than the $(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of $f$ itself. Similarly, for the non-monotone case, we devise another auxiliary function $F$ whose stationary points can achieve an optimal $\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation guarantee where $\\mathcal{C}$ is a convex constraint set. In contrast, the stationary points of the original non-monotone DR-submodular function can be arbitrarily bad~\\citep{chen2023continuous}. Furthermore, we demonstrate the scalability of our boosting technique on four problems. In all of these four problems, our resulting variants of boosting PGA algorithm beat the previous standard PGA in several aspects such as approximation ratio and efficiency. Finally, we corroborate our theoretical findings with numerical experiments, which demonstrate the effectiveness of our boosting PGA methods.","sentences":["Projected Gradient Ascent (PGA) is the most commonly used optimization scheme in machine learning and operations research areas.","Nevertheless, numerous studies and examples have shown that the PGA methods may fail to achieve the tight approximation ratio for continuous DR-submodular maximization problems.","To address this challenge, we present a boosting technique in this paper, which can efficiently improve the approximation guarantee of the standard PGA to \\emph{optimal} with only small modifications on the objective function.","The fundamental idea of our boosting technique is to exploit non-oblivious search to derive a novel auxiliary function $F$, whose stationary points are excellent approximations to the global maximum of the original DR-submodular objective $f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we propose an auxiliary function $F$ whose stationary points can provide a better $(1-e^{-\\gamma})$-approximation than the $(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of $f$ itself.","Similarly, for the non-monotone case, we devise another auxiliary function $F$ whose stationary points can achieve an optimal $\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation guarantee where $\\mathcal{C}$ is a convex constraint set.","In contrast, the stationary points of the original non-monotone DR-submodular function can be arbitrarily bad~\\citep{chen2023continuous}.","Furthermore, we demonstrate the scalability of our boosting technique on four problems.","In all of these four problems, our resulting variants of boosting PGA algorithm beat the previous standard PGA in several aspects such as approximation ratio and efficiency.","Finally, we corroborate our theoretical findings with numerical experiments, which demonstrate the effectiveness of our boosting PGA methods."],"url":"http://arxiv.org/abs/2401.08330v1"}
{"created":"2024-01-16 12:49:00","title":"Understanding User Experience in Large Language Model Interactions","abstract":"In the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence. This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration. This gap in research becomes increasingly critical as LLMs become more integrated into people's everyday and professional interactions. This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations. Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification. Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns. This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs. Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments. This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications.","sentences":["In the rapidly evolving landscape of large language models (LLMs), most research has primarily viewed them as independent individuals, focusing on assessing their capabilities through standardized benchmarks and enhancing their general intelligence.","This perspective, however, tends to overlook the vital role of LLMs as user-centric services in human-AI collaboration.","This gap in research becomes increasingly critical as LLMs become more integrated into people's everyday and professional interactions.","This study addresses the important need to understand user satisfaction with LLMs by exploring four key aspects: comprehending user intents, scrutinizing user experiences, addressing major user concerns about current LLM services, and charting future research paths to bolster human-AI collaborations.","Our study develops a taxonomy of 7 user intents in LLM interactions, grounded in analysis of real-world user interaction logs and human verification.","Subsequently, we conduct a user survey to gauge their satisfaction with LLM services, encompassing usage frequency, experiences across intents, and predominant concerns.","This survey, compiling 411 anonymous responses, uncovers 11 first-hand insights into the current state of user engagement with LLMs.","Based on this empirical analysis, we pinpoint 6 future research directions prioritizing the user perspective in LLM developments.","This user-centered approach is essential for crafting LLMs that are not just technologically advanced but also resonate with the intricate realities of human interactions and real-world applications."],"url":"http://arxiv.org/abs/2401.08329v1"}
{"created":"2024-01-16 12:48:52","title":"Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation","abstract":"In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions. This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing Test-Time Normalization Statistics\" (UnMix-TNS). UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment. The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch. Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts. UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples. Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks.","sentences":["In an era where test-time adaptation methods increasingly rely on the nuanced manipulation of batch normalization (BN) parameters, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.)","test batches with respect to unknown labels.","This assumption culminates in biased estimates of BN statistics and jeopardizes system stability under non-i.i.d. conditions.","This paper pioneers a departure from the i.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing Test-Time Normalization Statistics\" (UnMix-TNS).","UnMix-TNS re-calibrates the instance-wise statistics used to normalize each instance in a batch by mixing it with multiple unmixed statistics components, thus inherently simulating the i.i.d. environment.","The key lies in our innovative online unmixing procedure, which persistently refines these statistics components by drawing upon the closest instances from an incoming test batch.","Remarkably generic in its design, UnMix-TNS seamlessly integrates with an array of state-of-the-art test-time adaptation methods and pre-trained architectures equipped with BN layers.","Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios ranging from single to continual and mixed domain shifts.","UnMix-TNS stands out when handling test data streams with temporal correlation, including those with corrupted real-world non-i.i.d. streams, sustaining its efficacy even with minimal batch sizes and individual samples.","Our results set a new standard for test-time adaptation, demonstrating significant improvements in both stability and performance across multiple benchmarks."],"url":"http://arxiv.org/abs/2401.08328v1"}
{"created":"2024-01-16 12:45:15","title":"RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning","abstract":"Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.","sentences":["Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world.","Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world.","To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning.","Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling.","Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning.","For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy.","More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise.","In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning.","The code and data are available at https://github.com/Junjie-Ye/RoTBench."],"url":"http://arxiv.org/abs/2401.08326v1"}
{"created":"2024-01-16 12:45:15","title":"Learn What You Need in Personalized Federated Learning","abstract":"Personalized federated learning aims to address data heterogeneity across local clients in federated learning. However, current methods blindly incorporate either full model parameters or predefined partial parameters in personalized federated learning. They fail to customize the collaboration manner according to each local client's data characteristics, causing unpleasant aggregation results. To address this essential issue, we propose $\\textit{Learn2pFed}$, a novel algorithm-unrolling-based personalized federated learning framework, enabling each client to adaptively select which part of its local model parameters should participate in collaborative training. The key novelty of the proposed $\\textit{Learn2pFed}$ is to optimize each local model parameter's degree of participant in collaboration as learnable parameters via algorithm unrolling methods. This approach brings two benefits: 1) mathmatically determining the participation degree of local model parameters in the federated collaboration, and 2) obtaining more stable and improved solutions. Extensive experiments on various tasks, including regression, forecasting, and image classification, demonstrate that $\\textit{Learn2pFed}$ significantly outperforms previous personalized federated learning methods.","sentences":["Personalized federated learning aims to address data heterogeneity across local clients in federated learning.","However, current methods blindly incorporate either full model parameters or predefined partial parameters in personalized federated learning.","They fail to customize the collaboration manner according to each local client's data characteristics, causing unpleasant aggregation results.","To address this essential issue, we propose $\\textit{Learn2pFed}$, a novel algorithm-unrolling-based personalized federated learning framework, enabling each client to adaptively select which part of its local model parameters should participate in collaborative training.","The key novelty of the proposed $\\textit{Learn2pFed}$ is to optimize each local model parameter's degree of participant in collaboration as learnable parameters via algorithm unrolling methods.","This approach brings two benefits: 1) mathmatically determining the participation degree of local model parameters in the federated collaboration, and 2) obtaining more stable and improved solutions.","Extensive experiments on various tasks, including regression, forecasting, and image classification, demonstrate that $\\textit{Learn2pFed}$ significantly outperforms previous personalized federated learning methods."],"url":"http://arxiv.org/abs/2401.08327v1"}
{"created":"2024-01-16 12:36:17","title":"OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for Wideband Power Amplifier Modeling and Digital Pre-Distortion","abstract":"With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent. Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison. This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA DPA in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs. Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD.","sentences":["With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent.","Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison.","This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning.","We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA DPA in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs.","Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals.","OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD."],"url":"http://arxiv.org/abs/2401.08318v1"}
{"created":"2024-01-16 12:30:56","title":"Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening","abstract":"The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks. Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios. This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews. To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.","sentences":["The automation of resume screening is a crucial aspect of the recruitment process in organizations.","Automated resume screening systems often encompass a range of natural language processing (NLP) tasks.","The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks.","Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios.","This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes.","Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset.","Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews.","To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process.","Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis.","The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods.","Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\\%, during the resume sentence classification phase.","In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model.","Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes."],"url":"http://arxiv.org/abs/2401.08315v1"}
{"created":"2024-01-16 12:10:49","title":"Anchor function: a type of benchmark functions for studying language models","abstract":"Understanding transformer-based language models is becoming increasingly crucial, particularly as they play pivotal roles in advancing towards artificial general intelligence. However, language model research faces significant challenges, especially for academic research groups with constrained resources. These challenges include complex data structures, unknown target functions, high computational costs and memory requirements, and a lack of interpretability in the inference process, etc. Drawing a parallel to the use of simple models in scientific research, we propose the concept of an anchor function. This is a type of benchmark function designed for studying language models in learning tasks that follow an \"anchor-key\" pattern. By utilizing the concept of an anchor function, we can construct a series of functions to simulate various language tasks. The anchor function plays a role analogous to that of mice in diabetes research, particularly suitable for academic research. We demonstrate the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions. These operations are also commonly observed in large language models. The anchor function framework, therefore, opens up a series of valuable and accessible research questions for further exploration, especially for theoretical study.","sentences":["Understanding transformer-based language models is becoming increasingly crucial, particularly as they play pivotal roles in advancing towards artificial general intelligence.","However, language model research faces significant challenges, especially for academic research groups with constrained resources.","These challenges include complex data structures, unknown target functions, high computational costs and memory requirements, and a lack of interpretability in the inference process, etc.","Drawing a parallel to the use of simple models in scientific research, we propose the concept of an anchor function.","This is a type of benchmark function designed for studying language models in learning tasks that follow an \"anchor-key\" pattern.","By utilizing the concept of an anchor function, we can construct a series of functions to simulate various language tasks.","The anchor function plays a role analogous to that of mice in diabetes research, particularly suitable for academic research.","We demonstrate the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions.","These operations are also commonly observed in large language models.","The anchor function framework, therefore, opens up a series of valuable and accessible research questions for further exploration, especially for theoretical study."],"url":"http://arxiv.org/abs/2401.08309v1"}
{"created":"2024-01-16 11:50:54","title":"Evaluating online elasticity estimation of soft objects using standard robot grippers","abstract":"Standard robot grippers are not designed for elasticity estimation. In this work, a professional biaxial compression device was used as a control setup to study the accuracy with which material properties can be estimated by two standard parallel jaw grippers and a force/torque sensor mounted at the robot wrist. Using three sets of deformable objects, different parameters were varied to observe their effect on measuring material characteristics: (1) repeated compression cycles, (2) compression speed, and (3) the surface area of the gripper jaws. Gripper effort versus position curves were obtained and transformed into stress/strain curves. The modulus of elasticity was estimated at different strain points. Viscoelasticity was assessed using the energy absorbed in a compression/decompression cycle, the Kelvin-Voigt, and Hunt-Crossley models. Our results can be summarized as follows: (1) better results were obtained with slower compression speeds, while additional compression cycles or surface area did not improve estimation; (2) the robot grippers, even after calibration, were found to have a limited capability of delivering accurate estimates of absolute values of Young's modulus and viscoelasticity; (3) relative ordering of material characteristics was largely consistent across different grippers; (4) despite the nonlinear characteristics of deformable objects, fitting linear stress/strain approximations led to more stable results than local estimates of Young's modulus; (5) to assess viscoelasticity, the Hunt-Crossley model worked best. Finally, we show that a two-dimensional space representing elasticity and viscoelasticity estimates is advantageous for the discrimination of deformable objects. A single-grasp, online, classification and sorting of such objects is thus possible. An additional contribution is the dataset and data processing codes that we make publicly available.","sentences":["Standard robot grippers are not designed for elasticity estimation.","In this work, a professional biaxial compression device was used as a control setup to study the accuracy with which material properties can be estimated by two standard parallel jaw grippers and a force/torque sensor mounted at the robot wrist.","Using three sets of deformable objects, different parameters were varied to observe their effect on measuring material characteristics: (1) repeated compression cycles, (2) compression speed, and (3) the surface area of the gripper jaws.","Gripper effort versus position curves were obtained and transformed into stress/strain curves.","The modulus of elasticity was estimated at different strain points.","Viscoelasticity was assessed using the energy absorbed in a compression/decompression cycle, the Kelvin-Voigt, and Hunt-Crossley models.","Our results can be summarized as follows: (1) better results were obtained with slower compression speeds, while additional compression cycles or surface area did not improve estimation; (2) the robot grippers, even after calibration, were found to have a limited capability of delivering accurate estimates of absolute values of Young's modulus and viscoelasticity; (3) relative ordering of material characteristics was largely consistent across different grippers; (4) despite the nonlinear characteristics of deformable objects, fitting linear stress/strain approximations led to more stable results than local estimates of Young's modulus; (5) to assess viscoelasticity, the Hunt-Crossley model worked best.","Finally, we show that a two-dimensional space representing elasticity and viscoelasticity estimates is advantageous for the discrimination of deformable objects.","A single-grasp, online, classification and sorting of such objects is thus possible.","An additional contribution is the dataset and data processing codes that we make publicly available."],"url":"http://arxiv.org/abs/2401.08298v1"}
{"created":"2024-01-16 11:47:02","title":"The extension of zbMATH Open by arXiv preprints","abstract":"zbMATH Open has started a new feature -- relevant preprints posted at arXiv will also be displayed in the database. In this article we introduce this new feature and the underlying editorial policy. We also describe some of the technical issues involved and discuss the challenges this presents for future developments.","sentences":["zbMATH Open has started a new feature -- relevant preprints posted at arXiv will also be displayed in the database.","In this article we introduce this new feature and the underlying editorial policy.","We also describe some of the technical issues involved and discuss the challenges this presents for future developments."],"url":"http://arxiv.org/abs/2401.08297v1"}
{"created":"2024-01-16 11:45:03","title":"DAPT: A Dual Attention Framework for Parameter-Efficient Continual Learning of Large Language Models","abstract":"The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Based on parameter-efficient tuning (PET), existing methods devise the learning module and the selection module to handle the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in CL. The learning module allocates separate PET blocks for each continually emerged task and the selection module function to choose the correct one for the input at testing time. However, there are limitations in their deigns of both modules and they ignore the potential of aligning the two module to address CF and KT simultaneously. To this end, we propose a novel Dual Attention Framework , to align the PET learning and selection via the Dual Attentive Learning\\&Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT at the same time. Moreover, DAPT exhibits the superiority when we scale it to different model sizes (from 770M to 11B) and unseen tasks.","sentences":["The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world.","Based on parameter-efficient tuning (PET), existing methods devise the learning module and the selection module to handle the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in CL.","The learning module allocates separate PET blocks for each continually emerged task and the selection module function to choose the correct one for the input at testing time.","However, there are limitations in their deigns of both modules and they ignore the potential of aligning the two module to address CF and KT simultaneously.","To this end, we propose a novel Dual Attention Framework , to align the PET learning and selection via the Dual Attentive Learning\\&Selection module.","Extensive Experiments on two CL benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT at the same time.","Moreover, DAPT exhibits the superiority when we scale it to different model sizes (from 770M to 11B) and unseen tasks."],"url":"http://arxiv.org/abs/2401.08295v1"}
{"created":"2024-01-16 11:39:09","title":"Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models","abstract":"We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs). With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code. Compared with most existing inference engines, Inferflow has some key features. First, by implementing a modular framework of atomic build-blocks and technologies, Inferflow is compositionally generalizable to new models. Second, 3.5-bit quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit quantization. Third, hybrid model partitioning for multi-GPU inference is introduced in Inferflow to better balance inference speed and throughput than the existing partition-by-layer and partition-by-tensor strategies.","sentences":["We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs).","With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code.","Compared with most existing inference engines, Inferflow has some key features.","First, by implementing a modular framework of atomic build-blocks and technologies, Inferflow is compositionally generalizable to new models.","Second, 3.5-bit quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit quantization.","Third, hybrid model partitioning for multi-GPU inference is introduced in Inferflow to better balance inference speed and throughput than the existing partition-by-layer and partition-by-tensor strategies."],"url":"http://arxiv.org/abs/2401.08294v1"}
{"created":"2024-01-16 11:37:09","title":"ULT-model: Towards a one-legged unified locomotion template model for forward hopping with an upright trunk","abstract":"While many advancements have been made in the development of template models for describing upright-trunk locomotion, the majority of the effort has been focused on the stance phase. In this paper, we develop a new compact dynamic model as a first step toward a fully unified locomotion template model (ULT-model) of an upright-trunk forward hopping system, which will also require a unified control law in the next step. We demonstrate that all locomotion subfunctions are enabled by adding just a point foot mass and a parallel leg actuator to the well-known trunk SLIP model and that a stable limit cycle can be achieved. This brings us closer toward the ultimate goal of enabling closed-loop dynamics for anchor matching and thus achieving simple, efficient, robust and stable upright-trunk gait control, as observed in biological systems.","sentences":["While many advancements have been made in the development of template models for describing upright-trunk locomotion, the majority of the effort has been focused on the stance phase.","In this paper, we develop a new compact dynamic model as a first step toward a fully unified locomotion template model (ULT-model) of an upright-trunk forward hopping system, which will also require a unified control law in the next step.","We demonstrate that all locomotion subfunctions are enabled by adding just a point foot mass and a parallel leg actuator to the well-known trunk SLIP model and that a stable limit cycle can be achieved.","This brings us closer toward the ultimate goal of enabling closed-loop dynamics for anchor matching and thus achieving simple, efficient, robust and stable upright-trunk gait control, as observed in biological systems."],"url":"http://arxiv.org/abs/2401.08292v1"}
{"created":"2024-01-16 11:29:16","title":"RichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability Down to WebAssembly","abstract":"Safe, shared-memory interoperability between languages with different type systems and memory-safety guarantees is an intricate problem as crossing language boundaries may result in memory-safety violations. In this paper, we present RichWasm, a novel richly typed intermediate language designed to serve as a compilation target for typed high-level languages with different memory-safety guarantees. RichWasm is based on WebAssembly and enables safe shared-memory interoperability by incorporating a variety of type features that support fine-grained memory ownership and sharing. RichWasm is rich enough to serve as a typed compilation target for both typed garbage-collected languages and languages with an ownership-based type system and manually managed memory. We demonstrate this by providing compilers from core ML and L3, a type-safe language with strong updates, to RichWasm. RichWasm is compiled to regular Wasm, allowing for use in existing environments. We formalize RichWasm in Coq and prove type safety.","sentences":["Safe, shared-memory interoperability between languages with different type systems and memory-safety guarantees is an intricate problem as crossing language boundaries may result in memory-safety violations.","In this paper, we present RichWasm, a novel richly typed intermediate language designed to serve as a compilation target for typed high-level languages with different memory-safety guarantees.","RichWasm is based on WebAssembly and enables safe shared-memory interoperability by incorporating a variety of type features that support fine-grained memory ownership and sharing.","RichWasm is rich enough to serve as a typed compilation target for both typed garbage-collected languages and languages with an ownership-based type system and manually managed memory.","We demonstrate this by providing compilers from core ML and L3, a type-safe language with strong updates, to RichWasm.","RichWasm is compiled to regular Wasm, allowing for use in existing environments.","We formalize RichWasm in Coq and prove type safety."],"url":"http://arxiv.org/abs/2401.08287v1"}
{"created":"2024-01-16 11:15:34","title":"Nonlinear stiffness allows passive dynamic hopping for one-legged robots with an upright trunk","abstract":"Template models are frequently used to simplify the control dynamics for robot hopping or running. Passive limit cycles can emerge for such systems and be exploited for energy-efficient control. A grand challenge in locomotion is trunk stabilization when the hip is offset from the center of mass (CoM). The swing phase plays a major role in this process due to the moment of inertia of the leg; however, many template models ignore the leg mass. In this work, the authors consider a robot hopper model (RHM) with a rigid trunk and leg plus a hip that is displaced from the CoM. It has been previously shown that no passive limit cycle exists for such a model given a linear hip spring. In this work, we show that passive limit cycles can be found when a nonlinear hip spring is used instead. To the authors' knowledge, this is the first time that a passive limit cycle has been found for this type of system.","sentences":["Template models are frequently used to simplify the control dynamics for robot hopping or running.","Passive limit cycles can emerge for such systems and be exploited for energy-efficient control.","A grand challenge in locomotion is trunk stabilization when the hip is offset from the center of mass (CoM).","The swing phase plays a major role in this process due to the moment of inertia of the leg; however, many template models ignore the leg mass.","In this work, the authors consider a robot hopper model (RHM) with a rigid trunk and leg plus a hip that is displaced from the CoM. It has been previously shown that no passive limit cycle exists for such a model given a linear hip spring.","In this work, we show that passive limit cycles can be found when a nonlinear hip spring is used instead.","To the authors' knowledge, this is the first time that a passive limit cycle has been found for this type of system."],"url":"http://arxiv.org/abs/2401.08282v1"}
{"created":"2024-01-16 11:12:36","title":"The Faiss library","abstract":"Vector databases manage large collections of embedding vectors. As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.","sentences":["Vector databases manage large collections of embedding vectors.","As AI applications are growing rapidly, so are the number of embeddings that need to be stored and indexed.","The Faiss library is dedicated to vector similarity search, a core functionality of vector databases.","Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors.","This paper first describes the tradeoff space of vector search, then the design principles of Faiss in terms of structure, approach to optimization and interfacing.","We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability."],"url":"http://arxiv.org/abs/2401.08281v1"}
{"created":"2024-01-16 10:58:07","title":"AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception","abstract":"With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development. However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications. An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity. To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets. (1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts. (2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI). Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans. We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs. Source data will be available at https://github.com/yipoh/AesBench.","sentences":["With collective endeavors, multimodal large language models (MLLMs) are undergoing a flourishing development.","However, their performances on image aesthetics perception remain indeterminate, which is highly desired in real-world applications.","An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception.","This blind groping may impede the further development of more advanced MLLMs with aesthetic perception capacity.","To address this dilemma, we propose AesBench, an expert benchmark aiming to comprehensively evaluate the aesthetic perception capacities of MLLMs through elaborate design across dual facets.","(1) We construct an Expert-labeled Aesthetics Perception Database (EAPD), which features diversified image contents and high-quality annotations provided by professional aesthetic experts.","(2) We propose a set of integrative criteria to measure the aesthetic perception abilities of MLLMs from four perspectives, including Perception (AesP), Empathy (AesE), Assessment (AesA) and Interpretation (AesI).","Extensive experimental results underscore that the current MLLMs only possess rudimentary aesthetic perception ability, and there is still a significant gap between MLLMs and humans.","We hope this work can inspire the community to engage in deeper explorations on the aesthetic potentials of MLLMs.","Source data will be available at https://github.com/yipoh/AesBench."],"url":"http://arxiv.org/abs/2401.08276v1"}
{"created":"2024-01-16 10:54:37","title":"Modeling Spoof Noise by De-spoofing Diffusion and its Application in Face Anti-spoofing","abstract":"Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems. Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image. But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods. In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image. The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing. We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization.","sentences":["Face anti-spoofing is crucial for ensuring the security and reliability of face recognition systems.","Several existing face anti-spoofing methods utilize GAN-like networks to detect presentation attacks by estimating the noise pattern of a spoof image and recovering the corresponding genuine image.","But GAN's limited face appearance space results in the denoised faces cannot cover the full data distribution of genuine faces, thereby undermining the generalization performance of such methods.","In this work, we present a pioneering attempt to employ diffusion models to denoise a spoof image and restore the genuine image.","The difference between these two images is considered as the spoof noise, which can serve as a discriminative cue for face anti-spoofing.","We evaluate our proposed method on several intra-testing and inter-testing protocols, where the experimental results showcase the effectiveness of our method in achieving competitive performance in terms of both accuracy and generalization."],"url":"http://arxiv.org/abs/2401.08275v1"}
{"created":"2024-01-16 10:53:11","title":"Large Language Models are Null-Shot Learners","abstract":"This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the \"Examples\" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.","sentences":["This paper presents null-shot prompting.","Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the \"Examples\" section that never exists within the provided context to perform a task.","While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting.","Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering.","The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model.","These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets.","We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results."],"url":"http://arxiv.org/abs/2401.08273v1"}
{"created":"2024-01-16 10:51:55","title":"Siamese Content-based Search Engine for a More Transparent Skin and Breast Cancer Diagnosis through Histological Imaging","abstract":"Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making. Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features. In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor. The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs. The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy. Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists. To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images. The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels. So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics.","sentences":["Computer Aid Diagnosis (CAD) has developed digital pathology with Deep Learning (DL)-based tools to assist pathologists in decision-making.","Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek highly correlated patches in terms of similarity in histopathological features.","In this work, we proposed two CBHIR approaches on breast (Breast-twins) and skin cancer (Skin-twins) data sets for robust and accurate patch-level retrieval, integrating a custom-built Siamese network as a feature extractor.","The proposed Siamese network is able to generalize for unseen images by focusing on the similar histopathological features of the input pairs.","The proposed CBHIR approaches are evaluated on the Breast (public) and Skin (private) data sets with top K accuracy.","Finding the optimum amount of K is challenging, but also, as much as K increases, the dissimilarity between the query and the returned images increases which might mislead the pathologists.","To the best of the author's belief, this paper is tackling this issue for the first time on histopathological images by evaluating the top first retrieved images.","The Breast-twins model achieves 70% of the F1score at the top first, which exceeds the other state-of-the-art methods at a higher amount of K such as 5 and 400.","Skin-twins overpasses the recently proposed Convolutional Auto Encoder (CAE) by 67%, increasing the precision.","Besides, the Skin-twins model tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential (STUMP) to assist pathologists with retrieving top K images and their corresponding labels.","So, this approach can offer a more explainable CAD tool to pathologists in terms of transparency, trustworthiness, or reliability among other characteristics."],"url":"http://arxiv.org/abs/2401.08272v1"}
{"created":"2024-01-16 10:41:09","title":"Ranking Heterogeneous Search Result Pages using the Interactive Probability Ranking Principle","abstract":"The Probability Ranking Principle (PRP) ranks search results based on their expected utility derived solely from document contents, often overlooking the nuances of presentation and user interaction. However, with the evolution of Search Engine Result Pages (SERPs), now comprising a variety of result cards, the manner in which these results are presented is pivotal in influencing user engagement and satisfaction. This shift prompts the question: How does the PRP and its user-centric counterpart, the Interactive Probability Ranking Principle (iPRP), compare in the context of these heterogeneous SERPs? Our study draws a comparison between the PRP and the iPRP, revealing significant differences in their output. The iPRP, accounting for item-specific costs and interaction probabilities to determine the ``Expected Perceived Utility\" (EPU), yields different result orderings compared to the PRP. We evaluate the effect of the EPU on the ordering of results by observing changes in the ranking within a heterogeneous SERP compared to the traditional ``ten blue links''. We find that changing the presentation affects the ranking of items according to the (iPRP) by up to 48\\% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the TREC WaPo Collection. This work suggests that the iPRP should be employed when ranking heterogeneous SERPs to provide a user-centric ranking that adapts the ordering based on the presentation and user engagement.","sentences":["The Probability Ranking Principle (PRP) ranks search results based on their expected utility derived solely from document contents, often overlooking the nuances of presentation and user interaction.","However, with the evolution of Search Engine Result Pages (SERPs), now comprising a variety of result cards, the manner in which these results are presented is pivotal in influencing user engagement and satisfaction.","This shift prompts the question: How does the PRP and its user-centric counterpart, the Interactive Probability Ranking Principle (iPRP), compare in the context of these heterogeneous SERPs?","Our study draws a comparison between the PRP and the iPRP, revealing significant differences in their output.","The iPRP, accounting for item-specific costs and interaction probabilities to determine the ``Expected Perceived Utility\" (EPU), yields different result orderings compared to the PRP.","We evaluate the effect of the EPU on the ordering of results by observing changes in the ranking within a heterogeneous SERP compared to the traditional ``ten blue links''.","We find that changing the presentation affects the ranking of items according to the (iPRP) by up to 48\\% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the TREC WaPo Collection.","This work suggests that the iPRP should be employed when ranking heterogeneous SERPs to provide a user-centric ranking that adapts the ordering based on the presentation and user engagement."],"url":"http://arxiv.org/abs/2401.08267v1"}
{"created":"2024-01-16 10:35:59","title":"Towards a Transpiler for C/C++ to Safer Rust","abstract":"Rust is a multi-paradigm programming language developed by Mozilla that focuses on performance and safety. Rust code is arguably known best for its speed and memory safety, a property essential while developing embedded systems. Thus, it becomes one of the alternatives when developing operating systems for embedded devices. How to convert an existing C++ code base to Rust is also gaining greater attention. In this work, we focus on the process of transpiling C++ code to a Rust codebase in a robust and safe manner. The manual transpilation process is carried out to understand the different constructs of the Rust language and how they correspond to C++ constructs. Based on the learning from the manual transpilation, a transpilation table is created to aid in future transpilation efforts and to develop an automated transpiler. We also studied the existing automated transpilers and identified the problems and inefficiencies they involved. The results of the transpilation process were closely monitored and evaluated, showing improved memory safety without compromising performance and reliability of the resulting codebase. The study concludes with a comprehensive analysis of the findings, an evaluation of the implications for future research, and recommendations for the same in this area.","sentences":["Rust is a multi-paradigm programming language developed by Mozilla that focuses on performance and safety.","Rust code is arguably known best for its speed and memory safety, a property essential while developing embedded systems.","Thus, it becomes one of the alternatives when developing operating systems for embedded devices.","How to convert an existing C++ code base to Rust is also gaining greater attention.","In this work, we focus on the process of transpiling C++ code to a Rust codebase in a robust and safe manner.","The manual transpilation process is carried out to understand the different constructs of the Rust language and how they correspond to C++ constructs.","Based on the learning from the manual transpilation, a transpilation table is created to aid in future transpilation efforts and to develop an automated transpiler.","We also studied the existing automated transpilers and identified the problems and inefficiencies they involved.","The results of the transpilation process were closely monitored and evaluated, showing improved memory safety without compromising performance and reliability of the resulting codebase.","The study concludes with a comprehensive analysis of the findings, an evaluation of the implications for future research, and recommendations for the same in this area."],"url":"http://arxiv.org/abs/2401.08264v1"}
{"created":"2024-01-16 10:35:01","title":"Multi-Technique Sequential Information Consistency For Dynamic Visual Place Recognition In Changing Environments","abstract":"Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data. VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints. Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance. Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets. Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis. For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image. The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment.","sentences":["Visual place recognition (VPR) is an essential component of robot navigation and localization systems that allows them to identify a place using only image data.","VPR is challenging due to the significant changes in a place's appearance driven by different daily illumination, seasonal weather variations and diverse viewpoints.","Currently, no single VPR technique excels in every environmental condition, each exhibiting unique benefits and shortcomings, and therefore combining multiple techniques can achieve more reliable VPR performance.","Present multi-method approaches either rely on online ground-truth information, which is often not available, or on brute-force technique combination, potentially lowering performance with high variance technique sets.","Addressing these shortcomings, we propose a VPR system dubbed Multi-Sequential Information Consistency (MuSIC) which leverages sequential information to select the most cohesive technique on an online per-frame basis.","For each technique in a set, MuSIC computes their respective sequential consistencies by analysing the frame-to-frame continuity of their top match candidates, which are then directly compared to select the optimal technique for the current query image.","The use of sequential information to select between VPR methods results in an overall VPR performance increase across different benchmark datasets, while avoiding the need for extra ground-truth of the runtime environment."],"url":"http://arxiv.org/abs/2401.08263v1"}
{"created":"2024-01-16 10:32:13","title":"Probabilistically Robust Watermarking of Neural Networks","abstract":"As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups.","sentences":["As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model.","Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks.","In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation.","Our approach does not require additional model training and can be applied to any model architecture.","The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability.","In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model.","We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups."],"url":"http://arxiv.org/abs/2401.08261v1"}
{"created":"2024-01-16 10:28:12","title":"Time, Simultaneity, and Causality in Wireless Networks with Sensing and Communications","abstract":"Wireless systems beyond 5G evolve towards embracing both sensing and communication, resulting in increased convergence of the digital and the physical world. The existence of fused digital-physical realms raises critical questions regarding temporal ordering, causality, and the synchronization of events. This paper addresses the temporal challenges arising from the fact that the wireless infrastructure becomes an entity with multisensory perception. With the growing reliance on real-time interactions and applications such as digital twins, extended reality, and the metaverse, the need for accurate timestamping and temporal forensics becomes crucial. The paper introduces a model that incorporates Temporal Windows of Integration (TWI) to emulate human multisensory perception and discusses the implications for setting timing constraints in real-time applications and enabling temporal forensics. The analysis explores trade-offs, probabilities, and bounds for simultaneity and causality violation in the context of wireless systems evolving towards perceptive networks. This work underscores the significance of timestamping in the evolving wireless landscape, provide insights into system-level implications, and points out new research avenues for systems that combine sensing and communications.","sentences":["Wireless systems beyond 5G evolve towards embracing both sensing and communication, resulting in increased convergence of the digital and the physical world.","The existence of fused digital-physical realms raises critical questions regarding temporal ordering, causality, and the synchronization of events.","This paper addresses the temporal challenges arising from the fact that the wireless infrastructure becomes an entity with multisensory perception.","With the growing reliance on real-time interactions and applications such as digital twins, extended reality, and the metaverse, the need for accurate timestamping and temporal forensics becomes crucial.","The paper introduces a model that incorporates Temporal Windows of Integration (TWI) to emulate human multisensory perception and discusses the implications for setting timing constraints in real-time applications and enabling temporal forensics.","The analysis explores trade-offs, probabilities, and bounds for simultaneity and causality violation in the context of wireless systems evolving towards perceptive networks.","This work underscores the significance of timestamping in the evolving wireless landscape, provide insights into system-level implications, and points out new research avenues for systems that combine sensing and communications."],"url":"http://arxiv.org/abs/2401.08258v1"}
{"created":"2024-01-16 10:18:57","title":"Multitask Learning in Minimally Invasive Surgical Vision: A Review","abstract":"Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury. However, MIS poses additional complexity and burden on surgical teams. Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy. Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos. Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships. Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data. This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS. Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems. Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments.","sentences":["Minimally invasive surgery (MIS) has revolutionized many procedures and led to reduced recovery time and risk of patient injury.","However, MIS poses additional complexity and burden on surgical teams.","Data-driven surgical vision algorithms are thought to be key building blocks in the development of future MIS systems with improved autonomy.","Recent advancements in machine learning and computer vision have led to successful applications in analyzing videos obtained from MIS with the promise of alleviating challenges in MIS videos.","Surgical scene and action understanding encompasses multiple related tasks that, when solved individually, can be memory-intensive, inefficient, and fail to capture task relationships.","Multitask learning (MTL), a learning paradigm that leverages information from multiple related tasks to improve performance and aid generalization, is wellsuited for fine-grained and high-level understanding of MIS data.","This review provides an overview of the current state-of-the-art MTL systems that leverage videos obtained from MIS.","Beyond listing published approaches, we discuss the benefits and limitations of these MTL systems.","Moreover, this manuscript presents an analysis of the literature for various application fields of MTL in MIS, including those with large models, highlighting notable trends, new directions of research, and developments."],"url":"http://arxiv.org/abs/2401.08256v1"}
{"created":"2024-01-16 10:14:27","title":"A Generative Adversarial Attack for Multilingual Text Classifiers","abstract":"Current adversarial attack algorithms, where an adversary changes a text to fool a victim model, have been repeatedly shown to be effective against text classifiers. These attacks, however, generally assume that the victim model is monolingual and cannot be used to target multilingual victim models, a significant limitation given the increased use of these models. For this reason, in this work we propose an approach to fine-tune a multilingual paraphrase model with an adversarial objective so that it becomes able to generate effective adversarial examples against multilingual classifiers. The training objective incorporates a set of pre-trained models to ensure text quality and language consistency of the generated text. In addition, all the models are suitably connected to the generator by vocabulary-mapping matrices, allowing for full end-to-end differentiability of the overall training pipeline. The experimental validation over two multilingual datasets and five languages has shown the effectiveness of the proposed approach compared to existing baselines, particularly in terms of query efficiency. We also provide a detailed analysis of the generated attacks and discuss limitations and opportunities for future research.","sentences":["Current adversarial attack algorithms, where an adversary changes a text to fool a victim model, have been repeatedly shown to be effective against text classifiers.","These attacks, however, generally assume that the victim model is monolingual and cannot be used to target multilingual victim models, a significant limitation given the increased use of these models.","For this reason, in this work we propose an approach to fine-tune a multilingual paraphrase model with an adversarial objective so that it becomes able to generate effective adversarial examples against multilingual classifiers.","The training objective incorporates a set of pre-trained models to ensure text quality and language consistency of the generated text.","In addition, all the models are suitably connected to the generator by vocabulary-mapping matrices, allowing for full end-to-end differentiability of the overall training pipeline.","The experimental validation over two multilingual datasets and five languages has shown the effectiveness of the proposed approach compared to existing baselines, particularly in terms of query efficiency.","We also provide a detailed analysis of the generated attacks and discuss limitations and opportunities for future research."],"url":"http://arxiv.org/abs/2401.08255v1"}
{"created":"2024-01-16 10:04:29","title":"A techno-economic model for avoiding conflicts of interest between owners of offshore wind farms and maintenance suppliers","abstract":"Currently, wind energy is one of the most important sources of renewable energy. Offshore locations for wind turbines are increasingly exploited because of their numerous advantages. However, offshore wind farms require high investment in maintenance service. Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners. In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers. We created a complete techno-economic model to address this problem from an impartial point of view. An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives. Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests. The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier. This analysis evaluates the conflicts of interest of both parties. In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests.","sentences":["Currently, wind energy is one of the most important sources of renewable energy.","Offshore locations for wind turbines are increasingly exploited because of their numerous advantages.","However, offshore wind farms require high investment in maintenance service.","Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners.","In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers.","We created a complete techno-economic model to address this problem from an impartial point of view.","An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives.","Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests.","The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier.","This analysis evaluates the conflicts of interest of both parties.","In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests."],"url":"http://arxiv.org/abs/2401.08251v1"}
{"created":"2024-01-16 10:01:46","title":"Graph-based Algorithms for Linear Computation Coding","abstract":"We revisit existing linear computation coding (LCC) algorithms, and introduce a new framework that measures the computational cost of computing multidimensional linear functions, not only in terms of the number of additions, but also with respect to their suitability for parallel processing. Utilizing directed acyclic graphs, which correspond to signal flow graphs in hardware, we propose a novel LCC algorithm that controls the trade-off between the total number of operations and their parallel executability. Numerical evaluations show that the proposed algorithm, constrained to a fully parallel structure, outperforms existing schemes.","sentences":["We revisit existing linear computation coding (LCC) algorithms, and introduce a new framework that measures the computational cost of computing multidimensional linear functions, not only in terms of the number of additions, but also with respect to their suitability for parallel processing.","Utilizing directed acyclic graphs, which correspond to signal flow graphs in hardware, we propose a novel LCC algorithm that controls the trade-off between the total number of operations and their parallel executability.","Numerical evaluations show that the proposed algorithm, constrained to a fully parallel structure, outperforms existing schemes."],"url":"http://arxiv.org/abs/2401.08249v1"}
{"created":"2024-01-16 09:59:36","title":"Optimizing $k$ in $k$NN Graphs with Graph Learning Perspective","abstract":"In this paper, we propose a method, based on graph signal processing, to optimize the choice of $k$ in $k$-nearest neighbor graphs ($k$NNGs). $k$NN is one of the most popular approaches and is widely used in machine learning and signal processing. The parameter $k$ represents the number of neighbors that are connected to the target node; however, its appropriate selection is still a challenging problem. Therefore, most $k$NNGs use ad hoc selection methods for $k$. In the proposed method, we assume that a different $k$ can be chosen for each node. We formulate a discrete optimization problem to seek the best $k$ with a constraint on the sum of distances of the connected nodes. The optimal $k$ values are efficiently obtained without solving a complex optimization. Furthermore, we reveal that the proposed method is closely related to existing graph learning methods. In experiments on real datasets, we demonstrate that the $k$NNGs obtained with our method are sparse and can determine an appropriate variable number of edges per node. We validate the effectiveness of the proposed method for point cloud denoising, comparing our denoising performance with achievable graph construction methods that can be scaled to typical point cloud sizes (e.g., thousands of nodes).","sentences":["In this paper, we propose a method, based on graph signal processing, to optimize the choice of $k$ in $k$-nearest neighbor graphs ($k$NNGs).","$k$NN is one of the most popular approaches and is widely used in machine learning and signal processing.","The parameter $k$ represents the number of neighbors that are connected to the target node; however, its appropriate selection is still a challenging problem.","Therefore, most $k$NNGs use ad hoc selection methods for $k$. In the proposed method, we assume that a different $k$ can be chosen for each node.","We formulate a discrete optimization problem to seek the best $k$ with a constraint on the sum of distances of the connected nodes.","The optimal $k$ values are efficiently obtained without solving a complex optimization.","Furthermore, we reveal that the proposed method is closely related to existing graph learning methods.","In experiments on real datasets, we demonstrate that the $k$NNGs obtained with our method are sparse and can determine an appropriate variable number of edges per node.","We validate the effectiveness of the proposed method for point cloud denoising, comparing our denoising performance with achievable graph construction methods that can be scaled to typical point cloud sizes (e.g., thousands of nodes)."],"url":"http://arxiv.org/abs/2401.08245v1"}
{"created":"2024-01-16 09:53:48","title":"Polygonal Sequence-driven Triangulation Validator: An Incremental Approach to 2D Triangulation Verification","abstract":"Two-dimensional Delaunay triangulation is a fundamental aspect of computational geometry. This paper presents a novel algorithm that is specifically designed to ensure the correctness of 2D Delaunay triangulation, namely the Polygonal Sequence-driven Triangulation Validator (PSTV). Our research highlights the paramount importance of proper triangulation and the often overlooked, yet profound, impact of rounding errors in numerical computations on the precision of triangulation. The primary objective of the PSTV algorithm is to identify these computational errors and ensure the accuracy of the triangulation output. In addition to validating the correctness of triangulation, this study underscores the significance of the Delaunay property for the quality of finite element methods. Effective strategies are proposed to verify this property for a triangulation and correct it when necessary. While acknowledging the difficulty of rectifying complex triangulation errors such as overlapping triangles, these strategies provide valuable insights on identifying the locations of these errors and remedying them. The unique feature of the PSTV algorithm lies in its adoption of floating-point filters in place of interval arithmetic, striking an effective balance between computational efficiency and precision. This research sets a vital precedent for error reduction and precision enhancement in computational geometry.","sentences":["Two-dimensional Delaunay triangulation is a fundamental aspect of computational geometry.","This paper presents a novel algorithm that is specifically designed to ensure the correctness of 2D Delaunay triangulation, namely the Polygonal Sequence-driven Triangulation Validator (PSTV).","Our research highlights the paramount importance of proper triangulation and the often overlooked, yet profound, impact of rounding errors in numerical computations on the precision of triangulation.","The primary objective of the PSTV algorithm is to identify these computational errors and ensure the accuracy of the triangulation output.","In addition to validating the correctness of triangulation, this study underscores the significance of the Delaunay property for the quality of finite element methods.","Effective strategies are proposed to verify this property for a triangulation and correct it when necessary.","While acknowledging the difficulty of rectifying complex triangulation errors such as overlapping triangles, these strategies provide valuable insights on identifying the locations of these errors and remedying them.","The unique feature of the PSTV algorithm lies in its adoption of floating-point filters in place of interval arithmetic, striking an effective balance between computational efficiency and precision.","This research sets a vital precedent for error reduction and precision enhancement in computational geometry."],"url":"http://arxiv.org/abs/2401.08242v1"}
{"created":"2024-01-16 09:53:39","title":"Adapt/Exchange decisions or generic choices: Does framing influence how people integrate qualitatively different risks?","abstract":"In complex systems, decision makers often have to consider qualitatively different risks when choosing between options. Do their strategies of integrating these risks depend on the framing of problem contents? In the present study, participants were either instructed that they were choosing between two ways of solving a complex problem, or between two generic options. The former was framed as a modular plant scenario that required choices between modifying parameter settings in a current module (Adapt) and replacing the module by another one (Exchange). The risk was higher for Adapt to harm the product and for Exchange to harm the plant. These risks were presented as probabilities, and participants were either told that the consequences of both risks were equally severe (content-same group), or that harming the plant was much worse (content-different group). A third group made decisions based on the same probabilities, but received a generic task framing (no-content group). We expected framing to affect risk integration, leading the content-same group to make different choices than the no-content group. Contrary to this hypothesis, these two groups were strikingly similar in their decision outcomes and strategies, but clearly differed from the content-different group. These findings question whether ecological validity can be enhanced merely by framing a task in terms of real-world problem contents.","sentences":["In complex systems, decision makers often have to consider qualitatively different risks when choosing between options.","Do their strategies of integrating these risks depend on the framing of problem contents?","In the present study, participants were either instructed that they were choosing between two ways of solving a complex problem, or between two generic options.","The former was framed as a modular plant scenario that required choices between modifying parameter settings in a current module (Adapt) and replacing the module by another one (Exchange).","The risk was higher for Adapt to harm the product and for Exchange to harm the plant.","These risks were presented as probabilities, and participants were either told that the consequences of both risks were equally severe (content-same group), or that harming the plant was much worse (content-different group).","A third group made decisions based on the same probabilities, but received a generic task framing (no-content group).","We expected framing to affect risk integration, leading the content-same group to make different choices than the no-content group.","Contrary to this hypothesis, these two groups were strikingly similar in their decision outcomes and strategies, but clearly differed from the content-different group.","These findings question whether ecological validity can be enhanced merely by framing a task in terms of real-world problem contents."],"url":"http://arxiv.org/abs/2401.08241v1"}
{"created":"2024-01-16 09:49:19","title":"Phase-free Dynamic Movement Primitives Applied to Kinesthetic Guidance in Robotic Co-manipulation Tasks","abstract":"When there is a need to define and adapt a robotic task based on a reference motion, Dynamic Movement Primitives (DMP) is a standard and efficient method for encoding it. The nominal trajectory is typically obtained through a Programming by Demonstration (PbD) approach, where the robot is taught a specific task through kinesthetic guidance. Subsequently, the motion is reproduced by the manipulator in terms of both geometric path and timing law. The basic approach for modifying the duration of the execution involves adjusting a time constant characterizing the model. On the contrary, the goal of this paper is to achieve complete decoupling between the geometric information of the task, encoded into the DMP, and the phase law governing the execution, allowing them to be chosen independently. This enables the optimization of the task duration to satisfy constraints such as velocity or acceleration or even to define a phase law dependent on external inputs, such as the force applied by a user in a co-manipulation task. As an example, this mechanism will be exploited to define a rehabilitation activity where the cobot assists humans in performing various pre-planned exercises.","sentences":["When there is a need to define and adapt a robotic task based on a reference motion, Dynamic Movement Primitives (DMP) is a standard and efficient method for encoding it.","The nominal trajectory is typically obtained through a Programming by Demonstration (PbD) approach, where the robot is taught a specific task through kinesthetic guidance.","Subsequently, the motion is reproduced by the manipulator in terms of both geometric path and timing law.","The basic approach for modifying the duration of the execution involves adjusting a time constant characterizing the model.","On the contrary, the goal of this paper is to achieve complete decoupling between the geometric information of the task, encoded into the DMP, and the phase law governing the execution, allowing them to be chosen independently.","This enables the optimization of the task duration to satisfy constraints such as velocity or acceleration or even to define a phase law dependent on external inputs, such as the force applied by a user in a co-manipulation task.","As an example, this mechanism will be exploited to define a rehabilitation activity where the cobot assists humans in performing various pre-planned exercises."],"url":"http://arxiv.org/abs/2401.08238v1"}
{"created":"2024-01-16 09:44:09","title":"Interpreting Node Embedding Distances Through $n$-order Proximity Neighbourhoods","abstract":"In the field of node representation learning the task of interpreting latent dimensions has become a prominent, well-studied research topic. The contribution of this work focuses on appraising the interpretability of another rarely-exploited feature of node embeddings increasingly utilised in recommendation and consumption diversity studies: inter-node embedded distances. Introducing a new method to measure how understandable the distances between nodes are, our work assesses how well the proximity weights derived from a network before embedding relate to the node closeness measurements after embedding. Testing several classical node embedding models, our findings reach a conclusion familiar to practitioners albeit rarely cited in literature - the matrix factorisation model SVD is the most interpretable through 1, 2 and even higher-order proximities.","sentences":["In the field of node representation learning the task of interpreting latent dimensions has become a prominent, well-studied research topic.","The contribution of this work focuses on appraising the interpretability of another rarely-exploited feature of node embeddings increasingly utilised in recommendation and consumption diversity studies: inter-node embedded distances.","Introducing a new method to measure how understandable the distances between nodes are, our work assesses how well the proximity weights derived from a network before embedding relate to the node closeness measurements after embedding.","Testing several classical node embedding models, our findings reach a conclusion familiar to practitioners albeit rarely cited in literature - the matrix factorisation model SVD is the most interpretable through 1, 2 and even higher-order proximities."],"url":"http://arxiv.org/abs/2401.08236v1"}
{"created":"2024-01-16 09:34:17","title":"Enhancing Wind Speed and Wind Power Forecasting Using Shape-Wise Feature Engineering: A Novel Approach for Improved Accuracy and Robustness","abstract":"Accurate prediction of wind speed and power is vital for enhancing the efficiency of wind energy systems. Numerous solutions have been implemented to date, demonstrating their potential to improve forecasting. Among these, deep learning is perceived as a revolutionary approach in the field. However, despite their effectiveness, the noise present in the collected data remains a significant challenge. This noise has the potential to diminish the performance of these algorithms, leading to inaccurate predictions. In response to this, this study explores a novel feature engineering approach. This approach involves altering the data input shape in both Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various forecasting horizons. The results reveal substantial enhancements in model resilience against noise resulting from step increases in data. The approach could achieve an impressive 83% accuracy in predicting unseen data up to the 24th steps. Furthermore, this method consistently provides high accuracy for short, mid, and long-term forecasts, outperforming the performance of individual models. These findings pave the way for further research on noise reduction strategies at different forecasting horizons through shape-wise feature engineering.","sentences":["Accurate prediction of wind speed and power is vital for enhancing the efficiency of wind energy systems.","Numerous solutions have been implemented to date, demonstrating their potential to improve forecasting.","Among these, deep learning is perceived as a revolutionary approach in the field.","However, despite their effectiveness, the noise present in the collected data remains a significant challenge.","This noise has the potential to diminish the performance of these algorithms, leading to inaccurate predictions.","In response to this, this study explores a novel feature engineering approach.","This approach involves altering the data input shape in both Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various forecasting horizons.","The results reveal substantial enhancements in model resilience against noise resulting from step increases in data.","The approach could achieve an impressive 83% accuracy in predicting unseen data up to the 24th steps.","Furthermore, this method consistently provides high accuracy for short, mid, and long-term forecasts, outperforming the performance of individual models.","These findings pave the way for further research on noise reduction strategies at different forecasting horizons through shape-wise feature engineering."],"url":"http://arxiv.org/abs/2401.08233v1"}
{"created":"2024-01-16 09:33:29","title":"Multi-scale 2D Temporal Map Diffusion Models for Natural Language Video Localization","abstract":"Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding. Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data. In this study, we present a novel approach to NLVL that aims to address this issue. Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query. The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder. To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder. Our approach effectively encapsulates the interaction between the query and video data across various time scales. Experiments on the Charades and DiDeMo datasets underscore the potency of our design.","sentences":["Natural Language Video Localization (NLVL), grounding phrases from natural language descriptions to corresponding video segments, is a complex yet critical task in video understanding.","Despite ongoing advancements, many existing solutions lack the capability to globally capture temporal dynamics of the video data.","In this study, we present a novel approach to NLVL that aims to address this issue.","Our method involves the direct generation of a global 2D temporal map via a conditional denoising diffusion process, based on the input video and language query.","The main challenges are the inherent sparsity and discontinuity of a 2D temporal map in devising the diffusion decoder.","To address these challenges, we introduce a multi-scale technique and develop an innovative diffusion decoder.","Our approach effectively encapsulates the interaction between the query and video data across various time scales.","Experiments on the Charades and DiDeMo datasets underscore the potency of our design."],"url":"http://arxiv.org/abs/2401.08232v1"}
{"created":"2024-01-16 09:29:18","title":"Experimental Analysis of Type II Singularities and Assembly Change Points in a 3UPS+RPU Parallel Robot","abstract":"Parallel robots (PRs) have singular configurations where the robot gains at least one degree of freedom and loses control. Theoretically, such singularity occurs when the Forward Jacobian-matrix determinant becomes zero (Type II). However, actual PRs could lose control owing to Type II singularities for determinant values near zero, but not zero, because manufacturing tolerances introduce errors that are complex to model due to their low repeatability.   Thus, using an actual 3UPS+RPU PR, this paper presents three contributions: i) a proximity detection index for Type II singularities based on the angle between two Output Twist Screws. The index can identify which kinematic chains contribute to the singularity. ii) an experimental benchmark to study Type II singularities. iii) PR configurations where the proposed index is zero and the Forward Jacobian determinant is not. In this last configuration, the findings show that the actual robot is unable to handle external actions applied to the PR.","sentences":["Parallel robots (PRs) have singular configurations where the robot gains at least one degree of freedom and loses control.","Theoretically, such singularity occurs when the Forward Jacobian-matrix determinant becomes zero (Type II).","However, actual PRs could lose control owing to Type II singularities for determinant values near zero, but not zero, because manufacturing tolerances introduce errors that are complex to model due to their low repeatability.   ","Thus, using an actual 3UPS+RPU PR, this paper presents three contributions: i) a proximity detection index for Type II singularities based on the angle between two Output Twist Screws.","The index can identify which kinematic chains contribute to the singularity.","ii) an experimental benchmark to study Type II singularities.","iii) PR configurations where the proposed index is zero and the Forward Jacobian determinant is not.","In this last configuration, the findings show that the actual robot is unable to handle external actions applied to the PR."],"url":"http://arxiv.org/abs/2401.08229v1"}
{"created":"2024-01-16 09:27:28","title":"MCRPL: A Pretrain, Prompt & Fine-tune Paradigm for Non-overlapping Many-to-one Cross-domain Recommendation","abstract":"Cross-domain Recommendation (CR) is the task that tends to improve the recommendations in the sparse target domain by leveraging the information from other rich domains. Existing methods of cross-domain recommendation mainly focus on overlapping scenarios by assuming users are totally or partially overlapped, which are taken as bridges to connect different domains. However, this assumption does not always hold since it is illegal to leak users' identity information to other domains. Conducting Non-overlapping MCR (NMCR) is challenging since 1) The absence of overlapping information prevents us from directly aligning different domains, and this situation may get worse in the MCR scenario. 2) The distribution between source and target domains makes it difficult for us to learn common information across domains. To overcome the above challenges, we focus on NMCR, and devise MCRPL as our solution. To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts, and pre-train them in the pre-training stage. To address Challenge 2, we further update the domain-dependent prompts with other parameters kept fixed to transfer the domain knowledge to the target domain. We conduct experiments on five real-world domains, and the results show the advance of our MCRPL method compared with several recent SOTA baselines.","sentences":["Cross-domain Recommendation (CR) is the task that tends to improve the recommendations in the sparse target domain by leveraging the information from other rich domains.","Existing methods of cross-domain recommendation mainly focus on overlapping scenarios by assuming users are totally or partially overlapped, which are taken as bridges to connect different domains.","However, this assumption does not always hold since it is illegal to leak users' identity information to other domains.","Conducting Non-overlapping MCR (NMCR) is challenging since 1)","The absence of overlapping information prevents us from directly aligning different domains, and this situation may get worse in the MCR scenario.","2)","The distribution between source and target domains makes it difficult for us to learn common information across domains.","To overcome the above challenges, we focus on NMCR, and devise MCRPL as our solution.","To address Challenge 1, we first learn shared domain-agnostic and domain-dependent prompts, and pre-train them in the pre-training stage.","To address Challenge 2, we further update the domain-dependent prompts with other parameters kept fixed to transfer the domain knowledge to the target domain.","We conduct experiments on five real-world domains, and the results show the advance of our MCRPL method compared with several recent SOTA baselines."],"url":"http://arxiv.org/abs/2401.08228v1"}
{"created":"2024-01-16 09:24:51","title":"Core-periphery Detection Based on Masked Bayesian Non-negative Matrix Factorization","abstract":"Core-periphery structure is an essential mesoscale feature in complex networks. Previous researches mostly focus on discriminative approaches while in this work, we propose a generative model called masked Bayesian non-negative matrix factorization. We build the model using two pair affiliation matrices to indicate core-periphery pair associations and using a mask matrix to highlight connections to core nodes. We propose an approach to infer the model parameters, and prove the convergence of variables with our approach. Besides the abilities as traditional approaches, it is able to identify core scores with overlapping core-periphery pairs. We verify the effectiveness of our method using randomly generated networks and real-world networks. Experimental results demonstrate that the proposed method outperforms traditional approaches.","sentences":["Core-periphery structure is an essential mesoscale feature in complex networks.","Previous researches mostly focus on discriminative approaches while in this work, we propose a generative model called masked Bayesian non-negative matrix factorization.","We build the model using two pair affiliation matrices to indicate core-periphery pair associations and using a mask matrix to highlight connections to core nodes.","We propose an approach to infer the model parameters, and prove the convergence of variables with our approach.","Besides the abilities as traditional approaches, it is able to identify core scores with overlapping core-periphery pairs.","We verify the effectiveness of our method using randomly generated networks and real-world networks.","Experimental results demonstrate that the proposed method outperforms traditional approaches."],"url":"http://arxiv.org/abs/2401.08227v1"}
{"created":"2024-01-16 09:22:38","title":"Efficient and Mathematically Robust Operations for Certified Neural Networks Inference","abstract":"In recent years, machine learning (ML) and neural networks (NNs) have gained widespread use and attention across various domains, particularly in transportation for achieving autonomy, including the emergence of flying taxis for urban air mobility (UAM). However, concerns about certification have come up, compelling the development of standardized processes encompassing the entire ML and NN pipeline. This paper delves into the inference stage and the requisite hardware, highlighting the challenges associated with IEEE 754 floating-point arithmetic and proposing alternative number representations. By evaluating diverse summation and dot product algorithms, we aim to mitigate issues related to non-associativity. Additionally, our exploration of fixed-point arithmetic reveals its advantages over floating-point methods, demonstrating significant hardware efficiencies. Employing an empirical approach, we ascertain the optimal bit-width necessary to attain an acceptable level of accuracy, considering the inherent complexity of bit-width optimization.","sentences":["In recent years, machine learning (ML) and neural networks (NNs) have gained widespread use and attention across various domains, particularly in transportation for achieving autonomy, including the emergence of flying taxis for urban air mobility (UAM).","However, concerns about certification have come up, compelling the development of standardized processes encompassing the entire ML and NN pipeline.","This paper delves into the inference stage and the requisite hardware, highlighting the challenges associated with IEEE 754 floating-point arithmetic and proposing alternative number representations.","By evaluating diverse summation and dot product algorithms, we aim to mitigate issues related to non-associativity.","Additionally, our exploration of fixed-point arithmetic reveals its advantages over floating-point methods, demonstrating significant hardware efficiencies.","Employing an empirical approach, we ascertain the optimal bit-width necessary to attain an acceptable level of accuracy, considering the inherent complexity of bit-width optimization."],"url":"http://arxiv.org/abs/2401.08225v1"}
{"created":"2024-01-16 09:15:43","title":"Towards Causal Relationship in Indefinite Data: Baseline Model and New Datasets","abstract":"Integrating deep learning and causal discovery has encouraged us to spot that learning causal structures and representations in dialogue and video is full of challenges. We defined These data forms as \"Indefinite Data\", characterized by multi-structure data and multi-value representations. Unlike existing adaptable data forms, Indefinite Data still faces gaps in datasets and methods. To address the dataset gap, we release two high-quality datasets - Causalogue and Causaction, containing text dialogue samples and video action samples with causal annotations respectively. Moreover, the method gap arises from the coexistence of multi-structure data and multi-value representations, breaking the assumptions of all current methods and rendering them infeasible on Indefinite Data. To this end, we propose a probabilistic framework as a baseline, incorporating three designed highlights for this gap: 1) establishing Causation Condition of representations using the independence of noise terms under non-fixed causal structures, 2) treating causal strength as a latent variable and measuring the reconstruction loss in the correlation space, and 3) estimating the effects of latent confounders. These highpoints make the probabilistic model capable of overcoming challenges brought by the coexistence of multi-structure data and multi-value representations and pave the way for the extension of latent confounders. Comprehensive experiments have evaluated baseline results of causal structures, causal representations, and confounding disentanglement.","sentences":["Integrating deep learning and causal discovery has encouraged us to spot that learning causal structures and representations in dialogue and video is full of challenges.","We defined These data forms as \"Indefinite Data\", characterized by multi-structure data and multi-value representations.","Unlike existing adaptable data forms, Indefinite Data still faces gaps in datasets and methods.","To address the dataset gap, we release two high-quality datasets - Causalogue and Causaction, containing text dialogue samples and video action samples with causal annotations respectively.","Moreover, the method gap arises from the coexistence of multi-structure data and multi-value representations, breaking the assumptions of all current methods and rendering them infeasible on Indefinite Data.","To this end, we propose a probabilistic framework as a baseline, incorporating three designed highlights for this gap: 1) establishing Causation Condition of representations using the independence of noise terms under non-fixed causal structures, 2) treating causal strength as a latent variable and measuring the reconstruction loss in the correlation space, and 3) estimating the effects of latent confounders.","These highpoints make the probabilistic model capable of overcoming challenges brought by the coexistence of multi-structure data and multi-value representations and pave the way for the extension of latent confounders.","Comprehensive experiments have evaluated baseline results of causal structures, causal representations, and confounding disentanglement."],"url":"http://arxiv.org/abs/2401.08221v1"}
{"created":"2024-01-16 09:10:05","title":"Monoidal Extended Stone Duality","abstract":"Extensions of Stone-type dualities have a long history in algebraic logic and have also been instrumental for proving results in algebraic language theory. We show how to extend abstract categorical dualities via monoidal adjunctions, subsuming various incarnations of classical extended Stone and Priestley duality as a special case. Guided by these categorical foundations, we investigate residuation algebras, which are algebraic models of language derivatives, and show the subcategory of derivation algebras to be dually equivalent to the category of profinite ordered monoids, restricting to a duality between boolean residuation algebras and profinite monoids. We further extend this duality to capture relational morphisms of profinite ordered monoids, which dualize to natural morphisms of residuation algebras.","sentences":["Extensions of Stone-type dualities have a long history in algebraic logic and have also been instrumental for proving results in algebraic language theory.","We show how to extend abstract categorical dualities via monoidal adjunctions, subsuming various incarnations of classical extended Stone and Priestley duality as a special case.","Guided by these categorical foundations, we investigate residuation algebras, which are algebraic models of language derivatives, and show the subcategory of derivation algebras to be dually equivalent to the category of profinite ordered monoids, restricting to a duality between boolean residuation algebras and profinite monoids.","We further extend this duality to capture relational morphisms of profinite ordered monoids, which dualize to natural morphisms of residuation algebras."],"url":"http://arxiv.org/abs/2401.08219v1"}
{"created":"2024-01-16 09:04:17","title":"LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation","abstract":"As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests. To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks. By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability. We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets. The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications.","sentences":["As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests.","To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks.","By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability.","We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets.","The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications."],"url":"http://arxiv.org/abs/2401.08217v1"}
{"created":"2024-01-16 09:02:34","title":"Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning","abstract":"Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaining comparable recovery performance, we can accelerate the recovery speed and decrease memory consumption. Following this concept, we introduce Crab, an efficient and certified recovery method, which relies on selective information storage and adaptive model rollback. Theoretically, we demonstrate that the difference between the global model recovered by Crab and the one recovered by train-from-scratch can be bounded under certain assumptions. Our empirical evaluation, conducted across three datasets over multiple machine learning models, and a variety of untargeted and targeted poisoning attacks reveals that Crab is both accurate and efficient, and consistently outperforms previous approaches in terms of both recovery speed and memory consumption.","sentences":["Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model.","Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned.","Thus, a method is needed to recover an accurate global model after malicious clients are identified.","Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources.","In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model.","In this scenario, while maintaining comparable recovery performance, we can accelerate the recovery speed and decrease memory consumption.","Following this concept, we introduce Crab, an efficient and certified recovery method, which relies on selective information storage and adaptive model rollback.","Theoretically, we demonstrate that the difference between the global model recovered by Crab and the one recovered by train-from-scratch can be bounded under certain assumptions.","Our empirical evaluation, conducted across three datasets over multiple machine learning models, and a variety of untargeted and targeted poisoning attacks reveals that Crab is both accurate and efficient, and consistently outperforms previous approaches in terms of both recovery speed and memory consumption."],"url":"http://arxiv.org/abs/2401.08216v1"}
{"created":"2024-01-16 08:56:52","title":"Human vs. LMMs: Exploring the Discrepancy in Emoji Interpretation and Usage in Digital Communication","abstract":"Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications. Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions. Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction. This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis. The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures.","sentences":["Leveraging Large Multimodal Models (LMMs) to simulate human behaviors when processing multimodal information, especially in the context of social media, has garnered immense interest due to its broad potential and far-reaching implications.","Emojis, as one of the most unique aspects of digital communication, are pivotal in enriching and often clarifying the emotional and tonal dimensions.","Yet, there is a notable gap in understanding how these advanced models, such as GPT-4V, interpret and employ emojis in the nuanced context of online interaction.","This study intends to bridge this gap by examining the behavior of GPT-4V in replicating human-like use of emojis.","The findings reveal a discernible discrepancy between human and GPT-4V behaviors, likely due to the subjective nature of human interpretation and the limitations of GPT-4V's English-centric training, suggesting cultural biases and inadequate representation of non-English cultures."],"url":"http://arxiv.org/abs/2401.08212v1"}
{"created":"2024-01-16 08:54:21","title":"ModelNet-O: A Large-Scale Synthetic Dataset for Occlusion-Aware Point Cloud Classification","abstract":"Recently, 3D point cloud classification has made significant progress with the help of many datasets. However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods. To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras. ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods. Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner. We term our method PointMLS. Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective. More experiments also demonstrate the robustness and effectiveness of PointMLS.","sentences":["Recently, 3D point cloud classification has made significant progress with the help of many datasets.","However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion, which limits the practical application of current methods.","To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulate real-world point clouds with self-occlusion caused by scanning from monocular cameras.","ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods.","Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion, motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner.","We term our method PointMLS.","Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets, and it is robust and effective.","More experiments also demonstrate the robustness and effectiveness of PointMLS."],"url":"http://arxiv.org/abs/2401.08210v1"}
{"created":"2024-01-16 08:50:44","title":"Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary","abstract":"Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones. Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention. To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method. The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step. The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories. Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features. The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks.","sentences":["Single Image Super-Resolution is a classic computer vision problem that involves estimating high-resolution (HR) images from low-resolution (LR) ones.","Although deep neural networks (DNNs), especially Transformers for super-resolution, have seen significant advancements in recent years, challenges still remain, particularly in limited receptive field caused by window-based self-attention.","To address these issues, we introduce a group of auxiliary Adapeive Token Dictionary to SR Transformer and establish an ATD-SR method.","The introduced token dictionary could learn prior information from training data and adapt the learned prior to specific testing image through an adaptive refinement step.","The refinement strategy could not only provide global information to all input tokens but also group image tokens into categories.","Based on category partitions, we further propose a category-based self-attention mechanism designed to leverage distant but similar tokens for enhancing input features.","The experimental results show that our method achieves the best performance on various single image super-resolution benchmarks."],"url":"http://arxiv.org/abs/2401.08209v1"}
